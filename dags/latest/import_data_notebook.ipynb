{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "from zipfile import ZipFile\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Float, DateTime, Enum, ForeignKey, Boolean\n",
    "from sqlalchemy.orm import declarative_base, sessionmaker, relationship\n",
    "import enum\n",
    "import subprocess\n",
    "\n",
    "# Überprüfen und installieren Sie pymysql und cryptography, falls sie nicht vorhanden sind\n",
    "def ensure_dependencies_installed():\n",
    "    try:\n",
    "        import pymysql\n",
    "        print(\"pymysql ist bereits installiert.\")\n",
    "    except ImportError:\n",
    "        print(\"pymysql ist nicht installiert. Installation wird durchgeführt...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pymysql\"])\n",
    "        print(\"pymysql wurde erfolgreich installiert.\")\n",
    "    \n",
    "    try:\n",
    "        import cryptography\n",
    "        print(\"cryptography ist bereits installiert.\")\n",
    "    except ImportError:\n",
    "        print(\"cryptography ist nicht installiert. Installation wird durchgeführt...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"cryptography\"])\n",
    "        print(\"cryptography wurde erfolgreich installiert.\")\n",
    "\n",
    "# Lade die Konfigurationsdatei\n",
    "def load_config(config_file_path):\n",
    "    with open(config_file_path, 'r') as file:\n",
    "        config = json.load(file)\n",
    "    return config\n",
    "\n",
    "# Verbindungsschema für MySQL\n",
    "def create_db_engine(config):\n",
    "    ensure_dependencies_installed()  # Sicherstellen, dass pymysql und cryptography installiert sind\n",
    "    db_type = \"mysql+pymysql\"\n",
    "    url = f\"{db_type}://{config['db_user']}:{config['db_password']}@{config['db_host']}:{config['db_port']}/{config['db_name']}\"\n",
    "    return create_engine(url, echo=False)\n",
    "\n",
    "# Sitzung und Basis erstellen\n",
    "Base = declarative_base()\n",
    "\n",
    "# Enum für den Markt\n",
    "class Market(enum.Enum):\n",
    "    crypto = 'crypto'\n",
    "    stock = 'stock'\n",
    "    forex = 'forex'\n",
    "    futures = 'futures'\n",
    "\n",
    "# Tabellendefinitionen\n",
    "class Symbol(Base):\n",
    "    __tablename__ = 'symbol'\n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    ticker = Column(String(50), nullable=False)\n",
    "    name = Column(String(200), nullable=False)\n",
    "    market = Column(Enum(Market), nullable=False)\n",
    "    active = Column(Boolean, nullable=False)\n",
    "\n",
    "class MinuteBar(Base):\n",
    "    __tablename__ = 'minute_bar'\n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    date = Column(DateTime, nullable=False)\n",
    "    open = Column(Float)\n",
    "    high = Column(Float)\n",
    "    low = Column(Float)\n",
    "    close = Column(Float)\n",
    "    volume = Column(Float)\n",
    "    symbol_id = Column(Integer, ForeignKey('symbol.id', ondelete=\"CASCADE\"), nullable=False)\n",
    "    symbol = relationship('Symbol', backref='minute_bars')\n",
    "\n",
    "def extract_and_save_csv(zip_file_path, output_csv_path):\n",
    "    with ZipFile(zip_file_path) as zf:\n",
    "        cols = ['time', 'open', 'high', 'low', 'close', 'volume']\n",
    "        dfs = pd.concat({text_file.filename.split('.')[0]: pd.read_csv(zf.open(text_file.filename), usecols=cols)\n",
    "            for text_file in zf.infolist() if text_file.filename.endswith('.csv')})\n",
    "    \n",
    "    df = dfs.droplevel(1).reset_index().rename(columns={'index': 'ticker'})\n",
    "    df = df[df['ticker'].str.contains('usd')]\n",
    "    df['date'] = pd.to_datetime(df['time'], unit='ms')\n",
    "    df = df.sort_values(by=['date', 'ticker']).drop(columns='time').set_index(['date', 'ticker'])\n",
    "    \n",
    "    df.to_csv(output_csv_path)\n",
    "    print(f\"CSV-Datei wurde erfolgreich unter {output_csv_path} gespeichert.\")\n",
    "\n",
    "def load_data_from_csv(csv_file_path):\n",
    "    # Laden Sie die CSV-Datei und geben Sie die ersten Zeilen aus, um den Inhalt zu überprüfen\n",
    "    bars1m = pd.read_csv(csv_file_path)\n",
    "\n",
    "\n",
    "    # Überprüfen Sie, ob die erforderlichen Spalten vorhanden sind\n",
    "    if 'date' not in bars1m.columns or 'ticker' not in bars1m.columns:\n",
    "        print(\"CSV-Datei geladen. Erste Zeilen:\")\n",
    "        print(bars1m.head())\n",
    "        raise ValueError(\"Die CSV-Datei muss die Spalten 'date' und 'ticker' enthalten.\")\n",
    "\n",
    "    # Setzen des Index und Konvertieren des Datums\n",
    "    bars1m = bars1m.set_index(['date', 'ticker'])\n",
    "    bars1m.index = bars1m.index.set_levels([pd.to_datetime(bars1m.index.levels[0]), bars1m.index.levels[1]], level=['date', 'ticker'])\n",
    "    bars1m = bars1m.sort_index()\n",
    "    return bars1m\n",
    "\n",
    "def process_and_insert_data(session, bars1m, symbol_filter=None):\n",
    "    if symbol_filter:\n",
    "        bars1m = bars1m.query(f'ticker == \"{symbol_filter}\"')\n",
    "    \n",
    "    # Resample auf 1-Minuten-Intervalle\n",
    "    bars1m = bars1m.reset_index().set_index('date').groupby('ticker').resample('1min').last().droplevel(0)\n",
    "    bars1m.loc[:, bars1m.columns[:-1]] = bars1m[bars1m.columns[:-1]].ffill()\n",
    "    bars1m.loc[:, 'volume'] = bars1m['volume'].fillna(value=0.0)\n",
    "    bars1m = bars1m.reset_index().sort_values(by=['date', 'ticker']).set_index(['date', 'ticker'])\n",
    "    \n",
    "    tickers = bars1m.index.get_level_values(1).unique()\n",
    "    latest_date = bars1m.index.get_level_values('date').max()\n",
    "    active_tickers = bars1m.loc[latest_date].index.get_level_values('ticker').unique()\n",
    "    \n",
    "    symbols = pd.DataFrame(tickers, columns=['ticker'])\n",
    "    symbols['name'] = symbols['ticker']\n",
    "    symbols['market'] = 'crypto'\n",
    "    symbols['active'] = np.where(symbols['ticker'].isin(active_tickers), True, False)\n",
    "    symbols = symbols.sort_values(by='ticker')\n",
    "    \n",
    "    total_symbols = len(symbols)\n",
    "    for i, r in symbols.iterrows():\n",
    "        try:\n",
    "            print(f\"Uploading symbol {i+1}/{total_symbols}: {r['ticker']}\")\n",
    "            \n",
    "            symbol = Symbol(ticker=r['ticker'], name=r['name'], market=Market[r['market']], active=r['active'])\n",
    "            session.add(symbol)\n",
    "            session.commit()\n",
    "            \n",
    "            # Überprüfen, ob der Index existiert\n",
    "            if r['ticker'] in bars1m.index.get_level_values('ticker'):\n",
    "                bars = bars1m.xs(r['ticker'], level='ticker').reset_index()\n",
    "                bars['symbol_id'] = symbol.id\n",
    "                \n",
    "                session.bulk_insert_mappings(MinuteBar, bars.to_dict(orient='records'))\n",
    "                session.commit()\n",
    "            else:\n",
    "                print(f\"Ticker {r['ticker']} nicht im Index gefunden.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while uploading symbol {r['ticker']}: {e}\")\n",
    "            session.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: Usecols do not match columns, columns expected but not found: ['time']\n"
     ]
    }
   ],
   "source": [
    "def main_extract_and_save_csv():\n",
    "    try:\n",
    "        # Pfad zur config.json Datei\n",
    "        config_path = 'config.json'\n",
    "        \n",
    "        # Einlesen der Konfigurationsdatei\n",
    "        config = load_config(config_path)\n",
    "        \n",
    "        # Pfad zur Registry-Datei\n",
    "        registry_file_path = '/etc/airflow/airflow_dag_registry.yaml'\n",
    "        source_path = '/home/ageq/Git_Projects/MLdatalake/source'\n",
    "        \n",
    "        \n",
    "        # Erstellen des vollständigen Pfades zur ZIP-Datei\n",
    "        zip_file_name = config['zip_file_name']\n",
    "        \n",
    "        zip_file_path = os.path.join(source_path, f\"{zip_file_name}.zip\")\n",
    "        output_csv_path = os.path.join(source_path, f\"{zip_file_name}.csv\")\n",
    "        \n",
    "        # Entpacken und Speichern der CSV-Datei\n",
    "        extract_and_save_csv(zip_file_path, output_csv_path)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        \n",
    "main_extract_and_save_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pymysql ist bereits installiert.\n",
      "cryptography ist bereits installiert.\n",
      "Uploading symbol 6/113: adausd\n",
      "Uploading symbol 52/113: algusd\n",
      "Uploading symbol 39/113: ampusd\n",
      "Uploading symbol 57/113: antusd\n",
      "Uploading symbol 81/113: apeusd\n",
      "Uploading symbol 44/113: aptusd\n",
      "Uploading symbol 29/113: arbusd\n",
      "Uploading symbol 7/113: atousd\n",
      "Uploading symbol 76/113: axsusd\n",
      "Uploading symbol 107/113: b2musd\n",
      "Uploading symbol 68/113: balusd\n",
      "Uploading symbol 64/113: batusd\n",
      "Uploading symbol 30/113: bgbusd\n",
      "Uploading symbol 111/113: bmnusd\n",
      "Uploading symbol 78/113: bntusd\n",
      "Uploading symbol 8/113: btcusd\n",
      "Uploading symbol 9/113: bttusd\n",
      "Uploading symbol 22/113: ccdusd\n",
      "Uploading symbol 86/113: chzusd\n",
      "Uploading symbol 98/113: clousd\n",
      "Uploading symbol 69/113: crvusd\n",
      "Uploading symbol 93/113: daiusd\n",
      "Uploading symbol 31/113: dgbusd\n",
      "Uploading symbol 10/113: dotusd\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    try:\n",
    "        # Pfad zur config.json Datei\n",
    "        config_path = 'config.json'\n",
    "        \n",
    "        # Einlesen der Konfigurationsdatei\n",
    "        config = load_config(config_path)\n",
    "        \n",
    "        # Pfad zur Registry-Datei\n",
    "        registry_file_path = '/etc/airflow/airflow_dag_registry.yaml'\n",
    "        source_path = '/home/ageq/Git_Projects/MLdatalake/source'\n",
    "        \n",
    "        \n",
    "        # Erstellen des vollständigen Pfades zur ZIP-Datei\n",
    "        zip_file_name = config['zip_file_name']\n",
    "        \n",
    "        output_csv_path = os.path.join(source_path, f\"{zip_file_name}.csv\")\n",
    "\n",
    "        \n",
    "        # Erstellen der Datenbank-Engine\n",
    "        engine = create_db_engine(config)\n",
    "        Base.metadata.create_all(engine)\n",
    "        Session = sessionmaker(bind=engine)\n",
    "        session = Session()\n",
    "\n",
    "        # Laden der Daten aus der CSV-Datei\n",
    "        bars1m = load_data_from_csv(output_csv_path)\n",
    "        \n",
    "        #print(bars1m.head(5))\n",
    "        \n",
    "        # Optional: Filter für ein bestimmtes Symbol setzen\n",
    "        symbol_filter = None  # Beispiel: Nur Daten für BTCUSD importieren, setze symbol_filter = \"btcusd\"\n",
    "        process_and_insert_data(session, bars1m, symbol_filter)\n",
    "\n",
    "        print(\"Data imported successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##alternativer Zähler:\n",
    "def process_and_insert_data(session, bars1m, symbol_filter=None):\n",
    "    if symbol_filter:\n",
    "        bars1m = bars1m.query(f'ticker == \"{symbol_filter}\"')\n",
    "    \n",
    "    # Resample auf 1-Minuten-Intervalle\n",
    "    bars1m = bars1m.reset_index().set_index('date').groupby('ticker').resample('1min').last().droplevel(0)\n",
    "    bars1m.loc[:, bars1m.columns[:-1]] = bars1m[bars1m.columns[:-1]].ffill()\n",
    "    bars1m.loc[:, 'volume'] = bars1m['volume'].fillna(value=0.0)\n",
    "    bars1m = bars1m.reset_index().sort_values(by=['date', 'ticker']).set_index(['date', 'ticker'])\n",
    "    \n",
    "    tickers = bars1m.index.get_level_values(1).unique()\n",
    "    latest_date = bars1m.index.get_level_values('date').max()\n",
    "    active_tickers = bars1m.loc[latest_date].index.get_level_values('ticker').unique()\n",
    "    \n",
    "    symbols = pd.DataFrame(tickers, columns=['ticker'])\n",
    "    symbols['name'] = symbols['ticker']\n",
    "    symbols['market'] = 'crypto'\n",
    "    symbols['active'] = np.where(symbols['ticker'].isin(active_tickers), True, False)\n",
    "    symbols = symbols.sort_values(by='ticker')\n",
    "    \n",
    "    total_symbols = len(symbols)\n",
    "    for i, r in enumerate(symbols.itertuples(), 1):\n",
    "        try:\n",
    "            print(f\"Uploading symbol {i}/{total_symbols}: {r.ticker}\")\n",
    "            \n",
    "            symbol = Symbol(ticker=r.ticker, name=r.name, market=Market[r.market], active=r.active)\n",
    "            session.add(symbol)\n",
    "            session.commit()\n",
    "            \n",
    "            # Überprüfen, ob der Index existiert\n",
    "            if r.ticker in bars1m.index.get_level_values('ticker'):\n",
    "                bars = bars1m.xs(r.ticker, level='ticker').reset_index()\n",
    "                bars['symbol_id'] = symbol.id\n",
    "                \n",
    "                session.bulk_insert_mappings(MinuteBar, bars.to_dict(orient='records'))\n",
    "                session.commit()\n",
    "            else:\n",
    "                print(f\"Ticker {r.ticker} nicht im Index gefunden.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while uploading symbol {r.ticker}: {e}\")\n",
    "            session.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-03 18:59:54,872 INFO sqlalchemy.engine.Engine SELECT DATABASE()\n",
      "2024-10-03 18:59:54,873 INFO sqlalchemy.engine.Engine [raw sql] {}\n",
      "2024-10-03 18:59:54,876 INFO sqlalchemy.engine.Engine SELECT @@sql_mode\n",
      "2024-10-03 18:59:54,877 INFO sqlalchemy.engine.Engine [raw sql] {}\n",
      "2024-10-03 18:59:54,879 INFO sqlalchemy.engine.Engine SELECT @@lower_case_table_names\n",
      "2024-10-03 18:59:54,880 INFO sqlalchemy.engine.Engine [raw sql] {}\n",
      "2024-10-03 18:59:54,884 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2024-10-03 18:59:54,885 INFO sqlalchemy.engine.Engine SHOW FULL TABLES FROM `mldatalake`\n",
      "2024-10-03 18:59:54,886 INFO sqlalchemy.engine.Engine [raw sql] {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-03 18:59:54,890 INFO sqlalchemy.engine.Engine ROLLBACK\n",
      "['five_minute_bar', 'minute_bar', 'symbol', 'thirty_minute_bar']\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, inspect, Column, Integer, String, Float, DateTime, Enum, ForeignKey, Boolean\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "url = \"mysql+pymysql://mldatalake_user:userpassword@localhost:3308/mldatalake\"\n",
    "engine = create_engine(url, echo=True)\n",
    "\n",
    "inspector = inspect(engine)\n",
    "table_names = inspector.get_table_names()\n",
    "print(table_names)\n",
    "\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabellen in der Datenbank: ['five_minute_bar', 'minute_bar', 'symbol', 'thirty_minute_bar']\n",
      "Anzahl der Einträge in der Tabelle 'minute_bar': 17638421\n",
      "Date: 2023-10-08 09:28:00, Symbol: btcusd, Open: 27912.0, Close: 27912.0, High: 27912.0, Low: 27912.0, Volume: 0.00044152\n",
      "Date: 2023-10-08 09:28:00, Symbol: eosusd, Open: 0.5647, Close: 0.5647, High: 0.5647, Low: 0.5647, Volume: 125.433\n",
      "Date: 2023-10-08 09:28:00, Symbol: filusd, Open: 3.4256, Close: 3.427, High: 3.427, Low: 3.4256, Volume: 28.618\n",
      "Date: 2023-10-08 09:28:00, Symbol: iotusd, Open: 0.15376, Close: 0.15376, High: 0.15376, Low: 0.15376, Volume: 375.219\n",
      "Date: 2023-10-08 09:28:00, Symbol: etcusd, Open: 15.51, Close: 15.51, High: 15.51, Low: 15.51, Volume: 0.52338\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, inspect, Column, Integer, String, Float, DateTime, Enum, ForeignKey, Boolean\n",
    "from sqlalchemy.orm import declarative_base, sessionmaker, relationship\n",
    "import enum\n",
    "\n",
    "# Datenbankverbindung herstellen\n",
    "url = \"mysql+pymysql://mldatalake_user:userpassword@localhost:3308/mldatalake\"\n",
    "engine = create_engine(url, echo=False)\n",
    "\n",
    "# Inspektor verwenden, um Tabellen zu überprüfen\n",
    "inspector = inspect(engine)\n",
    "table_names = inspector.get_table_names()\n",
    "print(\"Tabellen in der Datenbank:\", table_names)\n",
    "\n",
    "# Basis für SQLAlchemy-Modelle definieren\n",
    "Base = declarative_base()\n",
    "\n",
    "# Enum für den Markt\n",
    "class Market(enum.Enum):\n",
    "    crypto = 'crypto'\n",
    "    stock = 'stock'\n",
    "    forex = 'forex'\n",
    "    futures = 'futures'\n",
    "\n",
    "# Modell für die Tabelle 'symbol' definieren\n",
    "class Symbol(Base):\n",
    "    __tablename__ = 'symbol'\n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    ticker = Column(String(50), nullable=False)\n",
    "    name = Column(String(200), nullable=False)\n",
    "    market = Column(Enum(Market), nullable=False)\n",
    "    active = Column(Boolean, nullable=False)\n",
    "\n",
    "# Modell für die Tabelle 'minute_bar' definieren\n",
    "class MinuteBar(Base):\n",
    "    __tablename__ = 'minute_bar'\n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    date = Column(DateTime, nullable=False)\n",
    "    open = Column(Float)\n",
    "    high = Column(Float)\n",
    "    low = Column(Float)\n",
    "    close = Column(Float)\n",
    "    volume = Column(Float)\n",
    "    symbol_id = Column(Integer, ForeignKey('symbol.id', ondelete=\"CASCADE\"), nullable=False)\n",
    "    symbol = relationship('Symbol', backref='minute_bars')\n",
    "\n",
    "# Sitzung erstellen\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "# Überprüfen, ob die Tabelle 'minute_bar' existiert\n",
    "if 'minute_bar' in table_names:\n",
    "    # Anzahl der Einträge in der Tabelle 'minute_bar' überprüfen\n",
    "    count = session.query(MinuteBar).count()\n",
    "    print(f\"Anzahl der Einträge in der Tabelle 'minute_bar': {count}\")\n",
    "\n",
    "    # Daten aus der Tabelle 'minute_bar' abrufen und die letzten 5 Einträge ausgeben\n",
    "    minute_bars = session.query(MinuteBar).order_by(MinuteBar.date.desc()).limit(5).all()\n",
    "    if minute_bars:\n",
    "        for bar in reversed(minute_bars):  # reversed, um die Einträge in aufsteigender Reihenfolge anzuzeigen\n",
    "            print(f\"Date: {bar.date}, Symbol: {bar.symbol.ticker}, Open: {bar.open}, Close: {bar.close}, High: {bar.high}, Low: {bar.low}, Volume: {bar.volume}\")\n",
    "    else:\n",
    "        print(\"Keine Einträge in der Tabelle 'minute_bar' gefunden.\")\n",
    "else:\n",
    "    print(\"Die Tabelle 'minute_bar' existiert nicht in der Datenbank.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import_data_notebook.ipynb\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from importlib import import_module\n",
    "\n",
    "# Ermitteln des aktuellen Verzeichnisses des Notebooks\n",
    "current_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "\n",
    "# Füge das Verzeichnis hinzu, in dem sich die Module befinden\n",
    "modules_dir = os.path.join(current_dir, 'modules')\n",
    "sys.path.append(modules_dir)\n",
    "\n",
    "# Importiere die Module\n",
    "create_database = import_module('create_database')\n",
    "extract_and_save_csv = import_module('extract_and_save_csv')\n",
    "import_to_db = import_module('import_to_db')\n",
    "\n",
    "# Lade die Konfigurationsdatei\n",
    "def load_config(config_file_path):\n",
    "    with open(config_file_path, 'r') as file:\n",
    "        config = json.load(file)\n",
    "    return config\n",
    "\n",
    "source = '/home/ageq/Git_Projects/MLdatalake/source/'\n",
    "\n",
    "# Beispiel für die Ausführung im Notebook\n",
    "config_file_path = os.path.join(current_dir, 'config.json')\n",
    "# Zielspeicherpfad\n",
    "csv_file_path = os.path.join(source, 'gespeicherter_dataframe.csv')\n",
    "symbol_filter = None  # Optional: Nur Daten für ein bestimmtes Symbol importieren, z.B. \"btcusd\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Erstelle die Datenbank und Tabellen\n",
    "create_database.main(config_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Extrahiere und speichere die CSV-Datei\n",
    "zip_file_path = os.path.join(source, 'trimmed_file.zip')\n",
    "extract_and_save_csv.extract_and_save_csv(zip_file_path, csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Importiere die Daten in die Datenbank\n",
    "import_to_db.main(config_file_path, csv_file_path, symbol_filter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
