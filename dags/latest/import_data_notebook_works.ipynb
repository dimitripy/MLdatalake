{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "from zipfile import ZipFile\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Float, DateTime, Enum, ForeignKey, Boolean\n",
    "from sqlalchemy.orm import declarative_base, sessionmaker, relationship\n",
    "import enum\n",
    "import subprocess\n",
    "\n",
    "# Überprüfen und installieren Sie pymysql und cryptography, falls sie nicht vorhanden sind\n",
    "def ensure_dependencies_installed():\n",
    "    try:\n",
    "        import pymysql\n",
    "        print(\"pymysql ist bereits installiert.\")\n",
    "    except ImportError:\n",
    "        print(\"pymysql ist nicht installiert. Installation wird durchgeführt...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pymysql\"])\n",
    "        print(\"pymysql wurde erfolgreich installiert.\")\n",
    "    \n",
    "    try:\n",
    "        import cryptography\n",
    "        print(\"cryptography ist bereits installiert.\")\n",
    "    except ImportError:\n",
    "        print(\"cryptography ist nicht installiert. Installation wird durchgeführt...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"cryptography\"])\n",
    "        print(\"cryptography wurde erfolgreich installiert.\")\n",
    "\n",
    "# Lade die Konfigurationsdatei\n",
    "def load_config(config_file_path):\n",
    "    with open(config_file_path, 'r') as file:\n",
    "        config = json.load(file)\n",
    "    return config\n",
    "\n",
    "# Verbindungsschema für MySQL\n",
    "def create_db_engine(config):\n",
    "    ensure_dependencies_installed()  # Sicherstellen, dass pymysql und cryptography installiert sind\n",
    "    db_type = \"mysql+pymysql\"\n",
    "    url = f\"{db_type}://{config['db_user']}:{config['db_password']}@{config['db_host']}:{config['db_port']}/{config['db_name']}\"\n",
    "    return create_engine(url, echo=False)\n",
    "\n",
    "# Sitzung und Basis erstellen\n",
    "Base = declarative_base()\n",
    "\n",
    "# Enum für den Markt\n",
    "class Market(enum.Enum):\n",
    "    crypto = 'crypto'\n",
    "    stock = 'stock'\n",
    "    forex = 'forex'\n",
    "    futures = 'futures'\n",
    "\n",
    "# Tabellendefinitionen\n",
    "class Symbol(Base):\n",
    "    __tablename__ = 'symbol'\n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    ticker = Column(String(50), nullable=False)\n",
    "    name = Column(String(200), nullable=False)\n",
    "    market = Column(Enum(Market), nullable=False)\n",
    "    active = Column(Boolean, nullable=False)\n",
    "\n",
    "class MinuteBar(Base):\n",
    "    __tablename__ = 'minute_bar'\n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    date = Column(DateTime, nullable=False)\n",
    "    open = Column(Float)\n",
    "    high = Column(Float)\n",
    "    low = Column(Float)\n",
    "    close = Column(Float)\n",
    "    volume = Column(Float)\n",
    "    symbol_id = Column(Integer, ForeignKey('symbol.id', ondelete=\"CASCADE\"), nullable=False)\n",
    "    symbol = relationship('Symbol', backref='minute_bars')\n",
    "\n",
    "def load_data_from_csv(csv_file_path):\n",
    "    # Laden Sie die CSV-Datei und geben Sie die ersten Zeilen aus, um den Inhalt zu überprüfen\n",
    "    bars1m = pd.read_csv(csv_file_path)\n",
    "\n",
    "\n",
    "    # Überprüfen Sie, ob die erforderlichen Spalten vorhanden sind\n",
    "    if 'date' not in bars1m.columns or 'ticker' not in bars1m.columns:\n",
    "        print(\"CSV-Datei geladen. Erste Zeilen:\")\n",
    "        print(bars1m.head())\n",
    "        raise ValueError(\"Die CSV-Datei muss die Spalten 'date' und 'ticker' enthalten.\")\n",
    "\n",
    "    # Setzen des Index und Konvertieren des Datums\n",
    "    bars1m = bars1m.set_index(['date', 'ticker'])\n",
    "    bars1m.index = bars1m.index.set_levels([pd.to_datetime(bars1m.index.levels[0]), bars1m.index.levels[1]], level=['date', 'ticker'])\n",
    "    bars1m = bars1m.sort_index()\n",
    "    return bars1m\n",
    "    \n",
    "def process_and_insert_data(session, bars1m, symbol_filter=None):\n",
    "    if symbol_filter:\n",
    "        bars1m = bars1m.query(f'ticker == \"{symbol_filter}\"')\n",
    "    \n",
    "    # Resample auf 1-Minuten-Intervalle\n",
    "    bars1m = bars1m.reset_index().set_index('date').groupby('ticker').resample('1min').last().droplevel(0)\n",
    "    bars1m.loc[:, bars1m.columns[:-1]] = bars1m[bars1m.columns[:-1]].ffill()\n",
    "    bars1m.loc[:, 'volume'] = bars1m['volume'].fillna(value=0.0)\n",
    "    bars1m = bars1m.reset_index().sort_values(by=['date', 'ticker']).set_index(['date', 'ticker'])\n",
    "    \n",
    "    tickers = bars1m.index.get_level_values(1).unique()\n",
    "    latest_date = bars1m.index.get_level_values('date').max()\n",
    "    active_tickers = bars1m.loc[latest_date].index.get_level_values('ticker').unique()\n",
    "    \n",
    "    symbols = pd.DataFrame(tickers, columns=['ticker'])\n",
    "    symbols['name'] = symbols['ticker']\n",
    "    symbols['market'] = 'crypto'\n",
    "    symbols['active'] = np.where(symbols['ticker'].isin(active_tickers), True, False)\n",
    "    symbols = symbols.sort_values(by='ticker')\n",
    "    \n",
    "    total_symbols = len(symbols)\n",
    "    for i, r in symbols.iterrows():\n",
    "        try:\n",
    "            print(f\"Uploading symbol {i+1}/{total_symbols}: {r['ticker']}\")\n",
    "            \n",
    "            symbol = Symbol(ticker=r['ticker'], name=r['name'], market=Market[r['market']], active=r['active'])\n",
    "            session.add(symbol)\n",
    "            session.commit()\n",
    "            \n",
    "            # Überprüfen, ob der Index existiert\n",
    "            if r['ticker'] in bars1m.index.get_level_values('ticker'):\n",
    "                bars = bars1m.xs(r['ticker'], level='ticker').reset_index()\n",
    "                bars['symbol_id'] = symbol.id\n",
    "                \n",
    "                session.bulk_insert_mappings(MinuteBar, bars.to_dict(orient='records'))\n",
    "                session.commit()\n",
    "            else:\n",
    "                print(f\"Ticker {r['ticker']} nicht im Index gefunden.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while uploading symbol {r['ticker']}: {e}\")\n",
    "            session.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZIP-Datei wurde erfolgreich nach /home/ageq/Git_Projects/MLdatalake/source/archive entpackt.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from zipfile import ZipFile\n",
    "\n",
    "def extract_zip(zip_file_path, extract_to_path):\n",
    "    with ZipFile(zip_file_path, 'r') as zf:\n",
    "        zf.extractall(extract_to_path)\n",
    "    print(f\"ZIP-Datei wurde erfolgreich nach {extract_to_path} entpackt.\")\n",
    "\n",
    "def main_extract_zip():\n",
    "    try:\n",
    "        # Pfad zur config.json Datei\n",
    "        config_path = 'config.json'\n",
    "        \n",
    "        # Einlesen der Konfigurationsdatei\n",
    "        config = load_config(config_path)\n",
    "        \n",
    "        # Pfad zur Registry-Datei\n",
    "        source_path = '/home/ageq/Git_Projects/MLdatalake/source'\n",
    "        \n",
    "        # Erstellen des vollständigen Pfades zur ZIP-Datei\n",
    "        zip_file_name = config['zip_file_name']\n",
    "        \n",
    "        zip_file_path = os.path.join(source_path, f\"{zip_file_name}.zip\")\n",
    "        extract_to_path = os.path.join(source_path, zip_file_name)\n",
    "        \n",
    "        # Entpacken der ZIP-Datei\n",
    "        extract_zip(zip_file_path, extract_to_path)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "main_extract_zip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def convert_csv_files(extract_to_path, output_csv_path):\n",
    "    cols = ['time', 'open', 'high', 'low', 'close', 'volume']\n",
    "    first_file = True\n",
    "    file_count = 0\n",
    "    \n",
    "    # Zählen der CSV-Dateien im Verzeichnis\n",
    "    total_files = sum([len(files) for r, d, files in os.walk(extract_to_path) if any(f.endswith('.csv') for f in files)])\n",
    "    \n",
    "    for root, dirs, files in os.walk(extract_to_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.csv'):\n",
    "                file_count += 1\n",
    "                print(f\"Verarbeite Datei {file_count} von {total_files}: {file}\")\n",
    "                #TODO - Dateigröße und zeitliche Dauer der Verarbeitung berechnen + Animation\n",
    "                file_path = os.path.join(root, file)\n",
    "                df = pd.read_csv(file_path, usecols=cols)\n",
    "                df['ticker'] = file.split('.')[0]\n",
    "                df = df[df['ticker'].str.contains('usd')]\n",
    "                df['date'] = pd.to_datetime(df['time'], unit='ms')\n",
    "                df = df.sort_values(by=['date', 'ticker']).drop(columns='time').set_index(['date', 'ticker'])\n",
    "                \n",
    "                if first_file:\n",
    "                    df.to_csv(output_csv_path, mode='w', header=True)\n",
    "                    first_file = False\n",
    "                else:\n",
    "                    df.to_csv(output_csv_path, mode='a', header=False)\n",
    "    \n",
    "    print(f\"CSV-Datei wurde erfolgreich unter {output_csv_path} gespeichert.\")\n",
    "\n",
    "def main_convert_csv():\n",
    "    try:\n",
    "        # Pfad zur config.json Datei\n",
    "        config_path = 'config.json'\n",
    "        \n",
    "        # Einlesen der Konfigurationsdatei\n",
    "        config = load_config(config_path)\n",
    "        \n",
    "        # Pfad zur Registry-Datei\n",
    "        source_path = '/home/ageq/Git_Projects/MLdatalake/source'\n",
    "        \n",
    "        # Erstellen des vollständigen Pfades zum Verzeichnis 'archive'\n",
    "        archive_path = os.path.join(source_path, 'archive')\n",
    "        output_csv_path = os.path.join(source_path, 'combined.csv')\n",
    "        \n",
    "        # Konvertieren der CSV-Dateien\n",
    "        convert_csv_files(archive_path, output_csv_path)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "main_convert_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mit counter\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def convert_csv_files(extract_to_path, output_csv_path):\n",
    "    cols = ['time', 'open', 'high', 'low', 'close', 'volume']\n",
    "    first_file = True\n",
    "    file_count = 0\n",
    "    \n",
    "    # Zählen der CSV-Dateien im Verzeichnis\n",
    "    total_files = sum([len(files) for r, d, files in os.walk(extract_to_path) if any(f.endswith('.csv') for f in files)])\n",
    "    \n",
    "    for root, dirs, files in os.walk(extract_to_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.csv'):\n",
    "                file_count += 1\n",
    "                print(f\"Verarbeite Datei {file_count} von {total_files}: {file}\")\n",
    "                \n",
    "                file_path = os.path.join(root, file)\n",
    "                df = pd.read_csv(file_path, usecols=cols)\n",
    "                df['ticker'] = file.split('.')[0]\n",
    "                df = df[df['ticker'].str.contains('usd')]\n",
    "                df['date'] = pd.to_datetime(df['time'], unit='ms')\n",
    "                df = df.sort_values(by=['date', 'ticker']).drop(columns='time').set_index(['date', 'ticker'])\n",
    "                \n",
    "                if first_file:\n",
    "                    df.to_csv(output_csv_path, mode='w', header=True)\n",
    "                    first_file = False\n",
    "                else:\n",
    "                    df.to_csv(output_csv_path, mode='a', header=False)\n",
    "    \n",
    "    print(f\"CSV-Datei wurde erfolgreich unter {output_csv_path} gespeichert.\")\n",
    "\n",
    "def main_convert_csv():\n",
    "    try:\n",
    "        # Pfad zur config.json Datei\n",
    "        config_path = 'config.json'\n",
    "        \n",
    "        # Einlesen der Konfigurationsdatei\n",
    "        config = load_config(config_path)\n",
    "        \n",
    "        # Pfad zur Registry-Datei\n",
    "        source_path = '/home/ageq/Git_Projects/MLdatalake/source'\n",
    "        \n",
    "        # Erstellen des vollständigen Pfades zum Verzeichnis 'archive'\n",
    "        archive_path = os.path.join(source_path, 'archive')\n",
    "        output_csv_path = os.path.join(source_path, 'combined.csv')\n",
    "        \n",
    "        # Konvertieren der CSV-Dateien\n",
    "        convert_csv_files(archive_path, output_csv_path)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "main_convert_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    try:\n",
    "        # Pfad zur config.json Datei\n",
    "        config_path = 'config.json'\n",
    "        \n",
    "        # Einlesen der Konfigurationsdatei\n",
    "        config = load_config(config_path)\n",
    "        \n",
    "        # Pfad zur Registry-Datei\n",
    "        registry_file_path = '/etc/airflow/airflow_dag_registry.yaml'\n",
    "        source_path = '/home/ageq/Git_Projects/MLdatalake/source'\n",
    "        \n",
    "        \n",
    "        # Erstellen des vollständigen Pfades zur ZIP-Datei\n",
    "        zip_file_name = config['zip_file_name']\n",
    "        \n",
    "        output_csv_path = os.path.join(source_path, f\"{zip_file_name}.csv\")\n",
    "\n",
    "        \n",
    "        # Erstellen der Datenbank-Engine\n",
    "        engine = create_db_engine(config)\n",
    "        Base.metadata.create_all(engine)\n",
    "        Session = sessionmaker(bind=engine)\n",
    "        session = Session()\n",
    "\n",
    "        # Laden der Daten aus der CSV-Datei\n",
    "        bars1m = load_data_from_csv(output_csv_path)\n",
    "        \n",
    "        #print(bars1m.head(5))\n",
    "        \n",
    "        # Optional: Filter für ein bestimmtes Symbol setzen\n",
    "        symbol_filter = None  # Beispiel: Nur Daten für BTCUSD importieren, setze symbol_filter = \"btcusd\"\n",
    "        process_and_insert_data(session, bars1m, symbol_filter)\n",
    "\n",
    "        print(\"Data imported successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##alternativer Zähler:\n",
    "def process_and_insert_data(session, bars1m, symbol_filter=None):\n",
    "    if symbol_filter:\n",
    "        bars1m = bars1m.query(f'ticker == \"{symbol_filter}\"')\n",
    "    \n",
    "    # Resample auf 1-Minuten-Intervalle\n",
    "    bars1m = bars1m.reset_index().set_index('date').groupby('ticker').resample('1min').last().droplevel(0)\n",
    "    bars1m.loc[:, bars1m.columns[:-1]] = bars1m[bars1m.columns[:-1]].ffill()\n",
    "    bars1m.loc[:, 'volume'] = bars1m['volume'].fillna(value=0.0)\n",
    "    bars1m = bars1m.reset_index().sort_values(by=['date', 'ticker']).set_index(['date', 'ticker'])\n",
    "    \n",
    "    tickers = bars1m.index.get_level_values(1).unique()\n",
    "    latest_date = bars1m.index.get_level_values('date').max()\n",
    "    active_tickers = bars1m.loc[latest_date].index.get_level_values('ticker').unique()\n",
    "    \n",
    "    symbols = pd.DataFrame(tickers, columns=['ticker'])\n",
    "    symbols['name'] = symbols['ticker']\n",
    "    symbols['market'] = 'crypto'\n",
    "    symbols['active'] = np.where(symbols['ticker'].isin(active_tickers), True, False)\n",
    "    symbols = symbols.sort_values(by='ticker')\n",
    "    \n",
    "    total_symbols = len(symbols)\n",
    "    for i, r in enumerate(symbols.itertuples(), 1):\n",
    "        try:\n",
    "            print(f\"Uploading symbol {i}/{total_symbols}: {r.ticker}\")\n",
    "            \n",
    "            symbol = Symbol(ticker=r.ticker, name=r.name, market=Market[r.market], active=r.active)\n",
    "            session.add(symbol)\n",
    "            session.commit()\n",
    "            \n",
    "            # Überprüfen, ob der Index existiert\n",
    "            if r.ticker in bars1m.index.get_level_values('ticker'):\n",
    "                bars = bars1m.xs(r.ticker, level='ticker').reset_index()\n",
    "                bars['symbol_id'] = symbol.id\n",
    "                \n",
    "                session.bulk_insert_mappings(MinuteBar, bars.to_dict(orient='records'))\n",
    "                session.commit()\n",
    "            else:\n",
    "                print(f\"Ticker {r.ticker} nicht im Index gefunden.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while uploading symbol {r.ticker}: {e}\")\n",
    "            session.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, inspect, Column, Integer, String, Float, DateTime, Enum, ForeignKey, Boolean\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "url = \"mysql+pymysql://mldatalake_user:userpassword@localhost:3308/mldatalake\"\n",
    "engine = create_engine(url, echo=True)\n",
    "\n",
    "inspector = inspect(engine)\n",
    "table_names = inspector.get_table_names()\n",
    "print(table_names)\n",
    "\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabellen in der Datenbank: ['five_minute_bar', 'minute_bar', 'symbol', 'thirty_minute_bar']\n",
      "Anzahl der Einträge in der Tabelle 'minute_bar': 17638421\n",
      "Date: 2023-10-08 09:28:00, Symbol: btcusd, Open: 27912.0, Close: 27912.0, High: 27912.0, Low: 27912.0, Volume: 0.00044152\n",
      "Date: 2023-10-08 09:28:00, Symbol: eosusd, Open: 0.5647, Close: 0.5647, High: 0.5647, Low: 0.5647, Volume: 125.433\n",
      "Date: 2023-10-08 09:28:00, Symbol: filusd, Open: 3.4256, Close: 3.427, High: 3.427, Low: 3.4256, Volume: 28.618\n",
      "Date: 2023-10-08 09:28:00, Symbol: iotusd, Open: 0.15376, Close: 0.15376, High: 0.15376, Low: 0.15376, Volume: 375.219\n",
      "Date: 2023-10-08 09:28:00, Symbol: etcusd, Open: 15.51, Close: 15.51, High: 15.51, Low: 15.51, Volume: 0.52338\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, inspect, Column, Integer, String, Float, DateTime, Enum, ForeignKey, Boolean\n",
    "from sqlalchemy.orm import declarative_base, sessionmaker, relationship\n",
    "import enum\n",
    "\n",
    "# Datenbankverbindung herstellen\n",
    "url = \"mysql+pymysql://mldatalake_user:userpassword@localhost:3308/mldatalake\"\n",
    "engine = create_engine(url, echo=False)\n",
    "\n",
    "# Inspektor verwenden, um Tabellen zu überprüfen\n",
    "inspector = inspect(engine)\n",
    "table_names = inspector.get_table_names()\n",
    "print(\"Tabellen in der Datenbank:\", table_names)\n",
    "\n",
    "# Basis für SQLAlchemy-Modelle definieren\n",
    "Base = declarative_base()\n",
    "\n",
    "# Enum für den Markt\n",
    "class Market(enum.Enum):\n",
    "    crypto = 'crypto'\n",
    "    stock = 'stock'\n",
    "    forex = 'forex'\n",
    "    futures = 'futures'\n",
    "\n",
    "# Modell für die Tabelle 'symbol' definieren\n",
    "class Symbol(Base):\n",
    "    __tablename__ = 'symbol'\n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    ticker = Column(String(50), nullable=False)\n",
    "    name = Column(String(200), nullable=False)\n",
    "    market = Column(Enum(Market), nullable=False)\n",
    "    active = Column(Boolean, nullable=False)\n",
    "\n",
    "# Modell für die Tabelle 'minute_bar' definieren\n",
    "class MinuteBar(Base):\n",
    "    __tablename__ = 'minute_bar'\n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    date = Column(DateTime, nullable=False)\n",
    "    open = Column(Float)\n",
    "    high = Column(Float)\n",
    "    low = Column(Float)\n",
    "    close = Column(Float)\n",
    "    volume = Column(Float)\n",
    "    symbol_id = Column(Integer, ForeignKey('symbol.id', ondelete=\"CASCADE\"), nullable=False)\n",
    "    symbol = relationship('Symbol', backref='minute_bars')\n",
    "\n",
    "# Sitzung erstellen\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "# Überprüfen, ob die Tabelle 'minute_bar' existiert\n",
    "if 'minute_bar' in table_names:\n",
    "    # Anzahl der Einträge in der Tabelle 'minute_bar' überprüfen\n",
    "    count = session.query(MinuteBar).count()\n",
    "    print(f\"Anzahl der Einträge in der Tabelle 'minute_bar': {count:_}\")\n",
    "\n",
    "    # Daten aus der Tabelle 'minute_bar' abrufen und die letzten 5 Einträge ausgeben\n",
    "    minute_bars = session.query(MinuteBar).order_by(MinuteBar.date.desc()).limit(5).all()\n",
    "    if minute_bars:\n",
    "        for bar in reversed(minute_bars):  # reversed, um die Einträge in aufsteigender Reihenfolge anzuzeigen\n",
    "            print(f\"Date: {bar.date}, Symbol: {bar.symbol.ticker}, Open: {bar.open}, Close: {bar.close}, High: {bar.high}, Low: {bar.low}, Volume: {bar.volume}\")\n",
    "    else:\n",
    "        print(\"Keine Einträge in der Tabelle 'minute_bar' gefunden.\")\n",
    "else:\n",
    "    print(\"Die Tabelle 'minute_bar' existiert nicht in der Datenbank.\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der Einträge: 2,000,000\n",
      "Doppelte Einträge in der CSV-Datei überprüfen:\n",
      "Keine doppelten Einträge gefunden.\n",
      "Effekt des Resampling auf die Anzahl der Einträge überprüfen:\n",
      "Originale Anzahl der Einträge: 2,000,000\n",
      "Anzahl der Einträge nach Resampling: 17,480,441\n"
     ]
    }
   ],
   "source": [
    "def check_for_duplicates(csv_file_path):\n",
    "    bars1m = pd.read_csv(csv_file_path)\n",
    "    duplicates = bars1m.duplicated(subset=['date', 'ticker'])\n",
    "    if duplicates.any():\n",
    "        print(f\"Doppelte Einträge gefunden: {duplicates.sum():,}\")\n",
    "        print(bars1m[duplicates])\n",
    "    else:\n",
    "        print(\"Keine doppelten Einträge gefunden.\")\n",
    "\n",
    "def check_resampling_effect(bars1m):\n",
    "    original_count = len(bars1m)\n",
    "    bars1m_resampled = bars1m.reset_index().set_index('date').groupby('ticker').resample('1min').last().droplevel(0)\n",
    "    resampled_count = len(bars1m_resampled)\n",
    "    print(f\"Originale Anzahl der Einträge: {original_count:_}\")\n",
    "    print(f\"Anzahl der Einträge nach Resampling: {resampled_count:_}\")\n",
    "\n",
    "# Anzahl der Einträge in der CSV-Datei überprüfen\n",
    "csv_file_path = '/home/ageq/Git_Projects/MLdatalake/source/trimmed_file.csv'\n",
    "\n",
    "# Laden der Daten aus der CSV-Datei\n",
    "bars1m = load_data_from_csv(csv_file_path)\n",
    "print(f\"Anzahl der Einträge: {len(bars1m):,}\")\n",
    "\n",
    "print(\"Doppelte Einträge in der CSV-Datei überprüfen:\")\n",
    "check_for_duplicates(csv_file_path)\n",
    "\n",
    "print(\"Effekt des Resampling auf die Anzahl der Einträge überprüfen:\")\n",
    "check_resampling_effect(bars1m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der Einträge in der Tabelle 'minute_bar': 17_638_421\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 25\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Duplikate in der Tabelle 'minute_bar' suchen\u001b[39;00m\n\u001b[1;32m     14\u001b[0m query \u001b[38;5;241m=\u001b[39m text(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124m    SELECT \u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124m        date, symbol_id, COUNT(*) as dupe_count\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124m        COUNT(*) > 1\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfetchall()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnzahl der Duplikate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(result)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Git_Projects/MLdatalake/venv/lib/python3.11/site-packages/sqlalchemy/orm/session.py:2362\u001b[0m, in \u001b[0;36mSession.execute\u001b[0;34m(self, statement, params, execution_options, bind_arguments, _parent_execute_state, _add_event)\u001b[0m\n\u001b[1;32m   2301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute\u001b[39m(\n\u001b[1;32m   2302\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2303\u001b[0m     statement: Executable,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2309\u001b[0m     _add_event: Optional[Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Result[Any]:\n\u001b[1;32m   2311\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Execute a SQL expression construct.\u001b[39;00m\n\u001b[1;32m   2312\u001b[0m \n\u001b[1;32m   2313\u001b[0m \u001b[38;5;124;03m    Returns a :class:`_engine.Result` object representing\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2360\u001b[0m \n\u001b[1;32m   2361\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_internal\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2363\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2364\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2365\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecution_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2366\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbind_arguments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbind_arguments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2367\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_parent_execute_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_parent_execute_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2368\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_add_event\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_add_event\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2369\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Git_Projects/MLdatalake/venv/lib/python3.11/site-packages/sqlalchemy/orm/session.py:2256\u001b[0m, in \u001b[0;36mSession._execute_internal\u001b[0;34m(self, statement, params, execution_options, bind_arguments, _parent_execute_state, _add_event, _scalar_result)\u001b[0m\n\u001b[1;32m   2247\u001b[0m     result: Result[Any] \u001b[38;5;241m=\u001b[39m compile_state_cls\u001b[38;5;241m.\u001b[39morm_execute_statement(\n\u001b[1;32m   2248\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2249\u001b[0m         statement,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2253\u001b[0m         conn,\n\u001b[1;32m   2254\u001b[0m     )\n\u001b[1;32m   2255\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2256\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecution_options\u001b[49m\n\u001b[1;32m   2258\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _scalar_result:\n\u001b[1;32m   2261\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mscalar()\n",
      "File \u001b[0;32m~/Git_Projects/MLdatalake/venv/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1418\u001b[0m, in \u001b[0;36mConnection.execute\u001b[0;34m(self, statement, parameters, execution_options)\u001b[0m\n\u001b[1;32m   1416\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObjectNotExecutableError(statement) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   1417\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1419\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdistilled_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mNO_OPTIONS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1422\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Git_Projects/MLdatalake/venv/lib/python3.11/site-packages/sqlalchemy/sql/elements.py:515\u001b[0m, in \u001b[0;36mClauseElement._execute_on_connection\u001b[0;34m(self, connection, distilled_params, execution_options)\u001b[0m\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, Executable)\n\u001b[0;32m--> 515\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_clauseelement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistilled_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_options\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObjectNotExecutableError(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/Git_Projects/MLdatalake/venv/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1640\u001b[0m, in \u001b[0;36mConnection._execute_clauseelement\u001b[0;34m(self, elem, distilled_parameters, execution_options)\u001b[0m\n\u001b[1;32m   1628\u001b[0m compiled_cache: Optional[CompiledCacheType] \u001b[38;5;241m=\u001b[39m execution_options\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1629\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompiled_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_compiled_cache\n\u001b[1;32m   1630\u001b[0m )\n\u001b[1;32m   1632\u001b[0m compiled_sql, extracted_params, cache_hit \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_compile_w_cache(\n\u001b[1;32m   1633\u001b[0m     dialect\u001b[38;5;241m=\u001b[39mdialect,\n\u001b[1;32m   1634\u001b[0m     compiled_cache\u001b[38;5;241m=\u001b[39mcompiled_cache,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1638\u001b[0m     linting\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdialect\u001b[38;5;241m.\u001b[39mcompiler_linting \u001b[38;5;241m|\u001b[39m compiler\u001b[38;5;241m.\u001b[39mWARN_LINTING,\n\u001b[1;32m   1639\u001b[0m )\n\u001b[0;32m-> 1640\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1641\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_ctx_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_compiled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1643\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompiled_sql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistilled_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompiled_sql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistilled_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1648\u001b[0m \u001b[43m    \u001b[49m\u001b[43melem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextracted_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_hit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_hit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_events:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mafter_execute(\n\u001b[1;32m   1654\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1655\u001b[0m         elem,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1659\u001b[0m         ret,\n\u001b[1;32m   1660\u001b[0m     )\n",
      "File \u001b[0;32m~/Git_Projects/MLdatalake/venv/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1846\u001b[0m, in \u001b[0;36mConnection._execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[1;32m   1844\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exec_insertmany_context(dialect, context)\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1846\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_exec_single_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Git_Projects/MLdatalake/venv/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1986\u001b[0m, in \u001b[0;36mConnection._exec_single_context\u001b[0;34m(self, dialect, context, statement, parameters)\u001b[0m\n\u001b[1;32m   1983\u001b[0m     result \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39m_setup_result_proxy()\n\u001b[1;32m   1985\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1986\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_dbapi_exception\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1987\u001b[0m \u001b[43m        \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_statement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m   1988\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1990\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Git_Projects/MLdatalake/venv/lib/python3.11/site-packages/sqlalchemy/engine/base.py:2358\u001b[0m, in \u001b[0;36mConnection._handle_dbapi_exception\u001b[0;34m(self, e, statement, parameters, cursor, context, is_sub_exec)\u001b[0m\n\u001b[1;32m   2356\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2357\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m exc_info[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2358\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc_info[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mwith_traceback(exc_info[\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m   2359\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2360\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reentrant_error\n",
      "File \u001b[0;32m~/Git_Projects/MLdatalake/venv/lib/python3.11/site-packages/sqlalchemy/engine/base.py:1967\u001b[0m, in \u001b[0;36mConnection._exec_single_context\u001b[0;34m(self, dialect, context, statement, parameters)\u001b[0m\n\u001b[1;32m   1965\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt_handled:\n\u001b[0;32m-> 1967\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_execute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1968\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_statement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m   1969\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1971\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_has_events:\n\u001b[1;32m   1972\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mafter_cursor_execute(\n\u001b[1;32m   1973\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1974\u001b[0m         cursor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1978\u001b[0m         context\u001b[38;5;241m.\u001b[39mexecutemany,\n\u001b[1;32m   1979\u001b[0m     )\n",
      "File \u001b[0;32m~/Git_Projects/MLdatalake/venv/lib/python3.11/site-packages/sqlalchemy/engine/default.py:941\u001b[0m, in \u001b[0;36mDefaultDialect.do_execute\u001b[0;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, cursor, statement, parameters, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 941\u001b[0m     \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Git_Projects/MLdatalake/venv/lib/python3.11/site-packages/pymysql/cursors.py:153\u001b[0m, in \u001b[0;36mCursor.execute\u001b[0;34m(self, query, args)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    151\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmogrify(query, args)\n\u001b[0;32m--> 153\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executed \u001b[38;5;241m=\u001b[39m query\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Git_Projects/MLdatalake/venv/lib/python3.11/site-packages/pymysql/cursors.py:322\u001b[0m, in \u001b[0;36mCursor._query\u001b[0;34m(self, q)\u001b[0m\n\u001b[1;32m    320\u001b[0m conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_db()\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_result()\n\u001b[0;32m--> 322\u001b[0m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_get_result()\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrowcount\n",
      "File \u001b[0;32m~/Git_Projects/MLdatalake/venv/lib/python3.11/site-packages/pymysql/connections.py:563\u001b[0m, in \u001b[0;36mConnection.query\u001b[0;34m(self, sql, unbuffered)\u001b[0m\n\u001b[1;32m    561\u001b[0m     sql \u001b[38;5;241m=\u001b[39m sql\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msurrogateescape\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_command(COMMAND\u001b[38;5;241m.\u001b[39mCOM_QUERY, sql)\n\u001b[0;32m--> 563\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_affected_rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_query_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43munbuffered\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munbuffered\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_affected_rows\n",
      "File \u001b[0;32m~/Git_Projects/MLdatalake/venv/lib/python3.11/site-packages/pymysql/connections.py:825\u001b[0m, in \u001b[0;36mConnection._read_query_result\u001b[0;34m(self, unbuffered)\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    824\u001b[0m     result \u001b[38;5;241m=\u001b[39m MySQLResult(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 825\u001b[0m     \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m result\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mserver_status \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Git_Projects/MLdatalake/venv/lib/python3.11/site-packages/pymysql/connections.py:1199\u001b[0m, in \u001b[0;36mMySQLResult.read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1198\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1199\u001b[0m         first_packet \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_packet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1201\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m first_packet\u001b[38;5;241m.\u001b[39mis_ok_packet():\n\u001b[1;32m   1202\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_ok_packet(first_packet)\n",
      "File \u001b[0;32m~/Git_Projects/MLdatalake/venv/lib/python3.11/site-packages/pymysql/connections.py:744\u001b[0m, in \u001b[0;36mConnection._read_packet\u001b[0;34m(self, packet_type)\u001b[0m\n\u001b[1;32m    742\u001b[0m buff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m()\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     packet_header \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    745\u001b[0m     \u001b[38;5;66;03m# if DEBUG: dump_packet(packet_header)\u001b[39;00m\n\u001b[1;32m    747\u001b[0m     btrl, btrh, packet_number \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39munpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<HBB\u001b[39m\u001b[38;5;124m\"\u001b[39m, packet_header)\n",
      "File \u001b[0;32m~/Git_Projects/MLdatalake/venv/lib/python3.11/site-packages/pymysql/connections.py:782\u001b[0m, in \u001b[0;36mConnection._read_bytes\u001b[0;34m(self, num_bytes)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 782\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rfile\u001b[38;5;241m.\u001b[39mread(num_bytes)\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##dauert zu lange\n",
    "\n",
    "from sqlalchemy import text\n",
    "\n",
    "# Sitzung erstellen\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "# Überprüfen, ob die Tabelle 'minute_bar' existiert\n",
    "if 'minute_bar' in table_names:\n",
    "    # Anzahl der Einträge in der Tabelle 'minute_bar' überprüfen\n",
    "    count = session.query(MinuteBar).count()\n",
    "    print(f\"Anzahl der Einträge in der Tabelle 'minute_bar': {count:_}\")\n",
    "\n",
    "    # Duplikate in der Tabelle 'minute_bar' suchen\n",
    "    query = text(\"\"\"\n",
    "        SELECT \n",
    "            date, symbol_id, COUNT(*) as dupe_count\n",
    "        FROM \n",
    "            minute_bar\n",
    "        GROUP BY \n",
    "            date, symbol_id\n",
    "        HAVING \n",
    "            COUNT(*) > 1\n",
    "    \"\"\")\n",
    "    \n",
    "    result = session.execute(query).fetchall()\n",
    "\n",
    "    if result:\n",
    "        print(f\"Anzahl der Duplikate: {len(result):_}\")\n",
    "        for row in result:\n",
    "            print(f\"Date: {row.date}, Symbol ID: {row.symbol_id}, Count: {row.dupe_coçunt}\")\n",
    "    else:\n",
    "        print(\"Keine Duplikate in der Tabelle 'minute_bar' gefunden.\")\n",
    "else:\n",
    "    print(\"Die Tabelle 'minute_bar' existiert nicht in der Datenbank.\")\n",
    "    \n",
    "    ##alternativ\n",
    "'''\n",
    "WITH recent_entries AS (\n",
    "    SELECT \n",
    "        date, \n",
    "        symbol_id\n",
    "    FROM \n",
    "        minute_bar\n",
    "    ORDER BY \n",
    "        date DESC\n",
    "    LIMIT 500000\n",
    ")\n",
    "SELECT \n",
    "    date, \n",
    "    symbol_id, \n",
    "    COUNT(*) as dupe_count\n",
    "FROM \n",
    "    recent_entries\n",
    "GROUP BY \n",
    "    date, \n",
    "    symbol_id\n",
    "HAVING \n",
    "    COUNT(*) > 1;\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import_data_notebook.ipynb\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from importlib import import_module\n",
    "\n",
    "# Ermitteln des aktuellen Verzeichnisses des Notebooks\n",
    "current_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "\n",
    "# Füge das Verzeichnis hinzu, in dem sich die Module befinden\n",
    "modules_dir = os.path.join(current_dir, 'modules')\n",
    "sys.path.append(modules_dir)\n",
    "\n",
    "# Importiere die Module\n",
    "create_database = import_module('create_database')\n",
    "extract_and_save_csv = import_module('extract_and_save_csv')\n",
    "import_to_db = import_module('import_to_db')\n",
    "\n",
    "# Lade die Konfigurationsdatei\n",
    "def load_config(config_file_path):\n",
    "    with open(config_file_path, 'r') as file:\n",
    "        config = json.load(file)\n",
    "    return config\n",
    "\n",
    "source = '/home/ageq/Git_Projects/MLdatalake/source/'\n",
    "\n",
    "# Beispiel für die Ausführung im Notebook\n",
    "config_file_path = os.path.join(current_dir, 'config.json')\n",
    "# Zielspeicherpfad\n",
    "csv_file_path = os.path.join(source, 'gespeicherter_dataframe.csv')\n",
    "symbol_filter = None  # Optional: Nur Daten für ein bestimmtes Symbol importieren, z.B. \"btcusd\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Erstelle die Datenbank und Tabellen\n",
    "create_database.main(config_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Extrahiere und speichere die CSV-Datei\n",
    "zip_file_path = os.path.join(source, 'trimmed_file.zip')\n",
    "extract_and_save_csv.extract_and_save_csv(zip_file_path, csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Importiere die Daten in die Datenbank\n",
    "import_to_db.main(config_file_path, csv_file_path, symbol_filter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
