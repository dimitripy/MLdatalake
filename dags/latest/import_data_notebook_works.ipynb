{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef load_and_process_data_from_csv(csv_file_path, chunksize=100000):\\n    # Initialisieren Sie eine leere Liste, um die verarbeiteten Chunks zu speichern\\n    processed_chunks = []\\n    \\n    # Laden Sie die CSV-Datei in Chunks\\n    for chunk in pd.read_csv(csv_file_path, chunksize=chunksize):\\n        # Überprüfen Sie, ob die erforderlichen Spalten vorhanden sind\\n        if \\'date\\' not in chunk.columns or \\'ticker\\' not in chunk.columns:\\n            print(\"CSV-Datei geladen. Erste Zeilen:\")\\n            print(chunk.head())\\n            raise ValueError(\"Die CSV-Datei muss die Spalten \\'date\\' und \\'ticker\\' enthalten.\")\\n        \\n        # Konvertieren des Datums und Setzen des Index\\n        chunk[\\'date\\'] = pd.to_datetime(chunk[\\'date\\'])\\n        chunk = chunk.set_index([\\'date\\', \\'ticker\\'])\\n        chunk = chunk.sort_index()\\n        \\n        # Fügen Sie den verarbeiteten Chunk zur Liste hinzu\\n        processed_chunks.append(chunk)\\n    \\n    return processed_chunks\\n\\n# Beispielaufruf\\ncsv_file_path = \\'path/to/your/large_csv_file.csv\\'\\nbars1m = load_data_from_csv(csv_file_path)\\nprint(bars1m.head())\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "from zipfile import ZipFile\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Float, DateTime, Enum, ForeignKey, Boolean\n",
    "from sqlalchemy.orm import declarative_base, sessionmaker, relationship\n",
    "import enum\n",
    "import subprocess\n",
    "\n",
    "# Überprüfen und installieren Sie pymysql und cryptography, falls sie nicht vorhanden sind\n",
    "def ensure_dependencies_installed():\n",
    "    try:\n",
    "        import pymysql\n",
    "        print(\"pymysql ist bereits installiert.\")\n",
    "    except ImportError:\n",
    "        print(\"pymysql ist nicht installiert. Installation wird durchgeführt...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pymysql\"])\n",
    "        print(\"pymysql wurde erfolgreich installiert.\")\n",
    "    \n",
    "    try:\n",
    "        import cryptography\n",
    "        print(\"cryptography ist bereits installiert.\")\n",
    "    except ImportError:\n",
    "        print(\"cryptography ist nicht installiert. Installation wird durchgeführt...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"cryptography\"])\n",
    "        print(\"cryptography wurde erfolgreich installiert.\")\n",
    "\n",
    "# Lade die Konfigurationsdatei\n",
    "def load_config(config_file_path):\n",
    "    with open(config_file_path, 'r') as file:\n",
    "        config = json.load(file)\n",
    "    return config\n",
    "\n",
    "# Verbindungsschema für MySQL\n",
    "def create_db_engine(config):\n",
    "    ensure_dependencies_installed()  # Sicherstellen, dass pymysql und cryptography installiert sind\n",
    "    db_type = \"mysql+pymysql\"\n",
    "    url = f\"{db_type}://{config['db_user']}:{config['db_password']}@{config['db_host']}:{config['db_port']}/{config['db_name']}\"\n",
    "    return create_engine(url, echo=False)\n",
    "\n",
    "# Sitzung und Basis erstellen\n",
    "Base = declarative_base()\n",
    "\n",
    "# Enum für den Markt\n",
    "class Market(enum.Enum):\n",
    "    crypto = 'crypto'\n",
    "    stock = 'stock'\n",
    "    forex = 'forex'\n",
    "    futures = 'futures'\n",
    "\n",
    "# Tabellendefinitionen\n",
    "class Symbol(Base):\n",
    "    __tablename__ = 'symbol'\n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    ticker = Column(String(50), nullable=False)\n",
    "    name = Column(String(200), nullable=False)\n",
    "    market = Column(Enum(Market), nullable=False)\n",
    "    active = Column(Boolean, nullable=False)\n",
    "\n",
    "class MinuteBar(Base):\n",
    "    __tablename__ = 'minute_bar'\n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    date = Column(DateTime, nullable=False)\n",
    "    open = Column(Float)\n",
    "    high = Column(Float)\n",
    "    low = Column(Float)\n",
    "    close = Column(Float)\n",
    "    volume = Column(Float)\n",
    "    symbol_id = Column(Integer, ForeignKey('symbol.id', ondelete=\"CASCADE\"), nullable=False)\n",
    "    symbol = relationship('Symbol', backref='minute_bars')\n",
    "'''\n",
    "def load_and_process_data_from_csv(csv_file_path, chunksize=100000):\n",
    "    # Initialisieren Sie eine leere Liste, um die verarbeiteten Chunks zu speichern\n",
    "    processed_chunks = []\n",
    "    \n",
    "    # Laden Sie die CSV-Datei in Chunks\n",
    "    for chunk in pd.read_csv(csv_file_path, chunksize=chunksize):\n",
    "        # Überprüfen Sie, ob die erforderlichen Spalten vorhanden sind\n",
    "        if 'date' not in chunk.columns or 'ticker' not in chunk.columns:\n",
    "            print(\"CSV-Datei geladen. Erste Zeilen:\")\n",
    "            print(chunk.head())\n",
    "            raise ValueError(\"Die CSV-Datei muss die Spalten 'date' und 'ticker' enthalten.\")\n",
    "        \n",
    "        # Konvertieren des Datums und Setzen des Index\n",
    "        chunk['date'] = pd.to_datetime(chunk['date'])\n",
    "        chunk = chunk.set_index(['date', 'ticker'])\n",
    "        chunk = chunk.sort_index()\n",
    "        \n",
    "        # Fügen Sie den verarbeiteten Chunk zur Liste hinzu\n",
    "        processed_chunks.append(chunk)\n",
    "    \n",
    "    return processed_chunks\n",
    "\n",
    "# Beispielaufruf\n",
    "csv_file_path = 'path/to/your/large_csv_file.csv'\n",
    "bars1m = load_data_from_csv(csv_file_path)\n",
    "print(bars1m.head())\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "def load_and_process_data_from_csv(csv_file_path, session, chunksize=100000, symbol_filter=None):\n",
    "    # Berechnen Sie die Gesamtzahl der Chunks\n",
    "    \n",
    "    print(\"pause\")\n",
    "    total_rows = sum(1 for _ in open(csv_file_path)) - 1  # Minus 1 für die Header-Zeile\n",
    "    print(total_rows)\n",
    "    total_chunks = (total_rows // chunksize) + 1\n",
    "    print(total_chunks)\n",
    "    \n",
    "    # Initialisieren Sie den Zähler für die verarbeiteten Chunks\n",
    "    processed_chunks = 0\n",
    "    \n",
    "    # Laden Sie die CSV-Datei in Chunks\n",
    "    for chunk in pd.read_csv(csv_file_path, chunksize=chunksize):\n",
    "        processed_chunks += 1\n",
    "        progress = (processed_chunks / total_chunks) * 100\n",
    "        print(f\"Verarbeite Chunk {processed_chunks}/{total_chunks} ({progress:.2f}%)\")\n",
    "        \n",
    "        # Überprüfen Sie, ob die erforderlichen Spalten vorhanden sind\n",
    "        if 'date' not in chunk.columns or 'ticker' not in chunk.columns:\n",
    "            print(\"CSV-Datei geladen. Erste Zeilen:\")\n",
    "            print(chunk.head())\n",
    "            raise ValueError(\"Die CSV-Datei muss die Spalten 'date' und 'ticker' enthalten.\")\n",
    "        \n",
    "        # Konvertieren des Datums und Setzen des Index\n",
    "        chunk['date'] = pd.to_datetime(chunk['date'])\n",
    "        chunk = chunk.set_index(['date', 'ticker'])\n",
    "        chunk = chunk.sort_index()\n",
    "        \n",
    "        # Optional: Filter für ein bestimmtes Symbol setzen\n",
    "        if symbol_filter:\n",
    "            chunk = chunk.query(f'ticker == \"{symbol_filter}\"')\n",
    "        \n",
    "        # Resample auf 1-Minuten-Intervalle\n",
    "        chunk = chunk.reset_index().set_index('date').groupby('ticker').resample('1min').last().droplevel(0)\n",
    "        chunk.loc[:, chunk.columns[:-1]] = chunk[chunk.columns[:-1]].ffill()\n",
    "        chunk.loc[:, 'volume'] = chunk['volume'].fillna(value=0.0)\n",
    "        chunk = chunk.reset_index().sort_values(by=['date', 'ticker']).set_index(['date', 'ticker'])\n",
    "        \n",
    "        tickers = chunk.index.get_level_values(1).unique()\n",
    "        latest_date = chunk.index.get_level_values('date').max()\n",
    "        active_tickers = chunk.loc[latest_date].index.get_level_values('ticker').unique()\n",
    "        \n",
    "        symbols = pd.DataFrame(tickers, columns=['ticker'])\n",
    "        symbols['name'] = symbols['ticker']\n",
    "        symbols['market'] = 'crypto'\n",
    "        symbols['active'] = np.where(symbols['ticker'].isin(active_tickers), True, False)\n",
    "        symbols = symbols.sort_values(by='ticker')\n",
    "        \n",
    "        total_symbols = len(symbols)\n",
    "        try:\n",
    "            for i, r in enumerate(symbols.itertuples(), 1):\n",
    "                symbol = Symbol(ticker=r.ticker, name=r.name, market=Market[r.market], active=r.active)\n",
    "                session.add(symbol)\n",
    "                \n",
    "                # Überprüfen, ob der Index existiert\n",
    "                if r.ticker in chunk.index.get_level_values('ticker'):\n",
    "                    bars = chunk.xs(r.ticker, level='ticker').reset_index()\n",
    "                    bars['symbol_id'] = symbol.id\n",
    "                    \n",
    "                    session.bulk_insert_mappings(MinuteBar, bars.to_dict(orient='records'))\n",
    "            \n",
    "            # Commit nach dem Verarbeiten des gesamten Chunks\n",
    "            session.commit()\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while uploading symbols: {e}\")\n",
    "            session.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alte funktion\n",
    "\n",
    "def process_and_insert_data(session, bars1m, symbol_filter=None):\n",
    "    if symbol_filter:\n",
    "        bars1m = bars1m.query(f'ticker == \"{symbol_filter}\"')\n",
    "    \n",
    "    # Resample auf 1-Minuten-Intervalle\n",
    "    bars1m = bars1m.reset_index().set_index('date').groupby('ticker').resample('1min').last().droplevel(0)\n",
    "    bars1m.loc[:, bars1m.columns[:-1]] = bars1m[bars1m.columns[:-1]].ffill()\n",
    "    bars1m.loc[:, 'volume'] = bars1m['volume'].fillna(value=0.0)\n",
    "    bars1m = bars1m.reset_index().sort_values(by=['date', 'ticker']).set_index(['date', 'ticker'])\n",
    "    \n",
    "    tickers = bars1m.index.get_level_values(1).unique()\n",
    "    latest_date = bars1m.index.get_level_values('date').max()\n",
    "    active_tickers = bars1m.loc[latest_date].index.get_level_values('ticker').unique()\n",
    "    \n",
    "    symbols = pd.DataFrame(tickers, columns=['ticker'])\n",
    "    symbols['name'] = symbols['ticker']\n",
    "    symbols['market'] = 'crypto'\n",
    "    symbols['active'] = np.where(symbols['ticker'].isin(active_tickers), True, False)\n",
    "    symbols = symbols.sort_values(by='ticker')\n",
    "    \n",
    "    total_symbols = len(symbols)\n",
    "    for i, r in symbols.iterrows():\n",
    "        try:\n",
    "            print(f\"Uploading symbol {i+1}/{total_symbols}: {r['ticker']}\")\n",
    "            \n",
    "            symbol = Symbol(ticker=r['ticker'], name=r['name'], market=Market[r['market']], active=r['active'])\n",
    "            session.add(symbol)\n",
    "            session.commit()\n",
    "            \n",
    "            # Überprüfen, ob der Index existiert\n",
    "            if r['ticker'] in bars1m.index.get_level_values('ticker'):\n",
    "                bars = bars1m.xs(r['ticker'], level='ticker').reset_index()\n",
    "                bars['symbol_id'] = symbol.id\n",
    "                \n",
    "                session.bulk_insert_mappings(MinuteBar, bars.to_dict(orient='records'))\n",
    "                session.commit()\n",
    "            else:\n",
    "                print(f\"Ticker {r['ticker']} nicht im Index gefunden.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while uploading symbol {r['ticker']}: {e}\")\n",
    "            session.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zip entpacken\n",
    "\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "\n",
    "def extract_zip(zip_file_path, extract_to_path):\n",
    "    with ZipFile(zip_file_path, 'r') as zf:\n",
    "        zf.extractall(extract_to_path)\n",
    "    print(f\"ZIP-Datei wurde erfolgreich nach {extract_to_path} entpackt.\")\n",
    "\n",
    "def main_extract_zip():\n",
    "    try:\n",
    "        # Pfad zur config.json Datei\n",
    "        config_path = 'config.json'\n",
    "        \n",
    "        # Einlesen der Konfigurationsdatei\n",
    "        config = load_config(config_path)\n",
    "        \n",
    "        # Pfad zur Registry-Datei\n",
    "        source_path = '/home/ageq/Git_Projects/MLdatalake/source'\n",
    "        \n",
    "        # Erstellen des vollständigen Pfades zur ZIP-Datei\n",
    "        zip_file_name = config['zip_file_name']\n",
    "        \n",
    "        zip_file_path = os.path.join(source_path, f\"{zip_file_name}.zip\")\n",
    "        extract_to_path = os.path.join(source_path, zip_file_name)\n",
    "        \n",
    "        # Entpacken der ZIP-Datei\n",
    "        extract_zip(zip_file_path, extract_to_path)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "main_extract_zip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verarbeite Datei 1 von 984: boousd.csv\n",
      "Verarbeite Datei 2 von 984: dshbtc.csv\n",
      "Verarbeite Datei 3 von 984: dotf0-ustf0.csv\n",
      "Verarbeite Datei 4 von 984: edoeth.csv\n",
      "Verarbeite Datei 5 von 984: gbpf0-ustf0.csv\n",
      "Verarbeite Datei 6 von 984: xmrf0_ustf0.csv\n",
      "Verarbeite Datei 7 von 984: reef-usd.csv\n",
      "Verarbeite Datei 8 von 984: dotf0btcf0.csv\n",
      "Verarbeite Datei 9 von 984: dogbtc.csv\n",
      "Verarbeite Datei 10 von 984: testapttestusd.csv\n",
      "Verarbeite Datei 11 von 984: avax-usd.csv\n",
      "Verarbeite Datei 12 von 984: neoeur.csv\n",
      "Verarbeite Datei 13 von 984: qshusd.csv\n",
      "Verarbeite Datei 14 von 984: ltcf0ustf0.csv\n",
      "Verarbeite Datei 15 von 984: flokiust.csv\n",
      "Verarbeite Datei 16 von 984: vsyusd.csv\n",
      "Verarbeite Datei 17 von 984: avax_ust.csv\n",
      "Verarbeite Datei 18 von 984: zilbtc.csv\n",
      "Verarbeite Datei 19 von 984: testfilf0_testusdtf0.csv\n",
      "Verarbeite Datei 20 von 984: mxntusd.csv\n",
      "Verarbeite Datei 21 von 984: japan225ixf0_ustf0.csv\n",
      "Verarbeite Datei 22 von 984: lymusd.csv\n",
      "Verarbeite Datei 23 von 984: wavesust.csv\n",
      "Verarbeite Datei 24 von 984: yywusd.csv\n",
      "Verarbeite Datei 25 von 984: cnh_cnht.csv\n",
      "Verarbeite Datei 26 von 984: blurusd.csv\n",
      "Verarbeite Datei 27 von 984: testeos_testusd.csv\n",
      "Verarbeite Datei 28 von 984: testethf0testusdtf0.csv\n",
      "Verarbeite Datei 29 von 984: ltcust.csv\n",
      "Verarbeite Datei 30 von 984: jpyf0_ustf0.csv\n",
      "Verarbeite Datei 31 von 984: sushiusd.csv\n",
      "Verarbeite Datei 32 von 984: hezusd.csv\n",
      "Verarbeite Datei 33 von 984: sandf0_ustf0.csv\n",
      "Verarbeite Datei 34 von 984: okbust.csv\n",
      "Verarbeite Datei 35 von 984: comp-usd.csv\n",
      "Verarbeite Datei 36 von 984: e.csv\n",
      "Verarbeite Datei 37 von 984: xtzbtc.csv\n",
      "Verarbeite Datei 38 von 984: dshusd.csv\n",
      "Verarbeite Datei 39 von 984: wildusd.csv\n",
      "Verarbeite Datei 40 von 984: bgbust.csv\n",
      "Verarbeite Datei 41 von 984: btcust.csv\n",
      "Verarbeite Datei 42 von 984: xautf0ustf0.csv\n",
      "Verarbeite Datei 43 von 984: utkusd.csv\n",
      "Verarbeite Datei 44 von 984: chsb_btc.csv\n",
      "Verarbeite Datei 45 von 984: ethf0_ustf0.csv\n",
      "Verarbeite Datei 46 von 984: xcad_usd.csv\n",
      "Verarbeite Datei 47 von 984: sweatusd.csv\n",
      "Verarbeite Datei 48 von 984: pnketh.csv\n",
      "Verarbeite Datei 49 von 984: arbf0ustf0.csv\n",
      "Verarbeite Datei 50 von 984: rrtusd.csv\n",
      "Verarbeite Datei 51 von 984: reef_ust.csv\n",
      "Verarbeite Datei 52 von 984: xdcust.csv\n",
      "Verarbeite Datei 53 von 984: btseusd.csv\n",
      "Verarbeite Datei 54 von 984: wminima_usd.csv\n",
      "Verarbeite Datei 55 von 984: polcust.csv\n",
      "Verarbeite Datei 56 von 984: sand_usd.csv\n",
      "Verarbeite Datei 57 von 984: pnkusd.csv\n",
      "Verarbeite Datei 58 von 984: unif0-ustf0.csv\n",
      "Verarbeite Datei 59 von 984: galaust.csv\n",
      "Verarbeite Datei 60 von 984: jasmyf0_ustf0.csv\n",
      "Verarbeite Datei 61 von 984: axsust.csv\n",
      "Verarbeite Datei 62 von 984: dtausd.csv\n",
      "Verarbeite Datei 63 von 984: ethf0-ustf0.csv\n",
      "Verarbeite Datei 64 von 984: testdogef0_testusdtf0.csv\n",
      "Verarbeite Datei 65 von 984: ksmusd.csv\n",
      "Verarbeite Datei 66 von 984: rrbusd.csv\n",
      "Verarbeite Datei 67 von 984: ldousd.csv\n",
      "Verarbeite Datei 68 von 984: reef-ust.csv\n",
      "Verarbeite Datei 69 von 984: convust.csv\n",
      "Verarbeite Datei 70 von 984: lunausd.csv\n",
      "Verarbeite Datei 71 von 984: icpf0_ustf0.csv\n",
      "Verarbeite Datei 72 von 984: xrausd.csv\n",
      "Verarbeite Datei 73 von 984: polcusd.csv\n",
      "Verarbeite Datei 74 von 984: neceth.csv\n",
      "Verarbeite Datei 75 von 984: xautf0btcf0.csv\n",
      "Verarbeite Datei 76 von 984: shft_ust.csv\n",
      "Verarbeite Datei 77 von 984: triusd.csv\n",
      "Verarbeite Datei 78 von 984: wavesusd.csv\n",
      "Verarbeite Datei 79 von 984: exrd-usd.csv\n",
      "Verarbeite Datei 80 von 984: testltctestusd.csv\n",
      "Verarbeite Datei 81 von 984: adaf0ustf0.csv\n",
      "Verarbeite Datei 82 von 984: vsybtc.csv\n",
      "Verarbeite Datei 83 von 984: icpbtc.csv\n",
      "Verarbeite Datei 84 von 984: wprusd.csv\n",
      "Verarbeite Datei 85 von 984: bchn_usd.csv\n",
      "Verarbeite Datei 86 von 984: zbtusd.csv\n",
      "Verarbeite Datei 87 von 984: trade_ust.csv\n",
      "Verarbeite Datei 88 von 984: treeb_ust.csv\n",
      "Verarbeite Datei 89 von 984: apenft_usd.csv\n",
      "Verarbeite Datei 90 von 984: neousd.csv\n",
      "Verarbeite Datei 91 von 984: chex_usd.csv\n",
      "Verarbeite Datei 92 von 984: sandust.csv\n",
      "Verarbeite Datei 93 von 984: dorausd.csv\n",
      "Verarbeite Datei 94 von 984: ognust.csv\n",
      "Verarbeite Datei 95 von 984: xlmf0ustf0.csv\n",
      "Verarbeite Datei 96 von 984: essusd.csv\n",
      "Verarbeite Datei 97 von 984: dogusd.csv\n",
      "Verarbeite Datei 98 von 984: shibust.csv\n",
      "Verarbeite Datei 99 von 984: b21x-ust.csv\n",
      "Verarbeite Datei 100 von 984: eurust.csv\n",
      "Verarbeite Datei 101 von 984: jstust.csv\n",
      "Verarbeite Datei 102 von 984: ampf0_ustf0.csv\n",
      "Verarbeite Datei 103 von 984: boson-ust.csv\n",
      "Verarbeite Datei 104 von 984: suku-ust.csv\n",
      "Verarbeite Datei 105 von 984: iqxusd.csv\n",
      "Verarbeite Datei 106 von 984: testltc_testusd.csv\n",
      "Verarbeite Datei 107 von 984: aptust.csv\n",
      "Verarbeite Datei 108 von 984: terraust-usd.csv\n",
      "Verarbeite Datei 109 von 984: veloust.csv\n",
      "Verarbeite Datei 110 von 984: btcdomf0ustf0.csv\n",
      "Verarbeite Datei 111 von 984: wavesf0_ustf0.csv\n",
      "Verarbeite Datei 112 von 984: dotf0-btcf0.csv\n",
      "Verarbeite Datei 113 von 984: xaut_ust.csv\n",
      "Verarbeite Datei 114 von 984: dora_usd.csv\n",
      "Verarbeite Datei 115 von 984: ethjpy.csv\n",
      "Verarbeite Datei 116 von 984: btcusd.csv\n",
      "Verarbeite Datei 117 von 984: xrpf0_ustf0.csv\n",
      "Verarbeite Datei 118 von 984: velo-usd.csv\n",
      "Verarbeite Datei 119 von 984: mimust.csv\n",
      "Verarbeite Datei 120 von 984: gmtusd.csv\n",
      "Verarbeite Datei 121 von 984: crvusd.csv\n",
      "Verarbeite Datei 122 von 984: xagf0_ustf0.csv\n",
      "Verarbeite Datei 123 von 984: xdcusd.csv\n",
      "Verarbeite Datei 124 von 984: leoeth.csv\n",
      "Verarbeite Datei 125 von 984: etcf0ustf0.csv\n",
      "Verarbeite Datei 126 von 984: ltcusd.csv\n",
      "Verarbeite Datei 127 von 984: europe50ixf0ustf0.csv\n",
      "Verarbeite Datei 128 von 984: briseust.csv\n",
      "Verarbeite Datei 129 von 984: xaut-btc.csv\n",
      "Verarbeite Datei 130 von 984: mkrf0_ustf0.csv\n",
      "Verarbeite Datei 131 von 984: stjusd.csv\n",
      "Verarbeite Datei 132 von 984: jpyust.csv\n",
      "Verarbeite Datei 133 von 984: onlusd.csv\n",
      "Verarbeite Datei 134 von 984: dusk-usd.csv\n",
      "Verarbeite Datei 135 von 984: ethf0-btcf0.csv\n",
      "Verarbeite Datei 136 von 984: gntusd.csv\n",
      "Verarbeite Datei 137 von 984: eth_xaut.csv\n",
      "Verarbeite Datei 138 von 984: xrpbtc.csv\n",
      "Verarbeite Datei 139 von 984: crvf0_ustf0.csv\n",
      "Verarbeite Datei 140 von 984: adaust.csv\n",
      "Verarbeite Datei 141 von 984: bateth.csv\n",
      "Verarbeite Datei 142 von 984: xmrbtc.csv\n",
      "Verarbeite Datei 143 von 984: prmx_usd.csv\n",
      "Verarbeite Datei 144 von 984: exrd-btc.csv\n",
      "Verarbeite Datei 145 von 984: qrdo_ust.csv\n",
      "Verarbeite Datei 146 von 984: luna-ust.csv\n",
      "Verarbeite Datei 147 von 984: forthust.csv\n",
      "Verarbeite Datei 148 von 984: repbtc.csv\n",
      "Verarbeite Datei 149 von 984: mkrusd.csv\n",
      "Verarbeite Datei 150 von 984: avax_btc.csv\n",
      "Verarbeite Datei 151 von 984: idxust.csv\n",
      "Verarbeite Datei 152 von 984: aave-ust.csv\n",
      "Verarbeite Datei 153 von 984: bsvusd.csv\n",
      "Verarbeite Datei 154 von 984: ustcnht.csv\n",
      "Verarbeite Datei 155 von 984: xlmeth.csv\n",
      "Verarbeite Datei 156 von 984: xpdf0_ustf0.csv\n",
      "Verarbeite Datei 157 von 984: etcbtc.csv\n",
      "Verarbeite Datei 158 von 984: hezust.csv\n",
      "Verarbeite Datei 159 von 984: btceut.csv\n",
      "Verarbeite Datei 160 von 984: etpbtc.csv\n",
      "Verarbeite Datei 161 von 984: spellusd.csv\n",
      "Verarbeite Datei 162 von 984: chsb_ust.csv\n",
      "Verarbeite Datei 163 von 984: eusbtc.csv\n",
      "Verarbeite Datei 164 von 984: ethw_ust.csv\n",
      "Verarbeite Datei 165 von 984: testbtcf0testusdtf0.csv\n",
      "Verarbeite Datei 166 von 984: avaxust.csv\n",
      "Verarbeite Datei 167 von 984: dgxusd.csv\n",
      "Verarbeite Datei 168 von 984: apenft_ust.csv\n",
      "Verarbeite Datei 169 von 984: arbf0_ustf0.csv\n",
      "Verarbeite Datei 170 von 984: oxyusd.csv\n",
      "Verarbeite Datei 171 von 984: gxtust.csv\n",
      "Verarbeite Datei 172 von 984: velo-ust.csv\n",
      "Verarbeite Datei 173 von 984: zcnusd.csv\n",
      "Verarbeite Datei 174 von 984: testalgotestusd.csv\n",
      "Verarbeite Datei 175 von 984: avaxbtc.csv\n",
      "Verarbeite Datei 176 von 984: luna2usd.csv\n",
      "Verarbeite Datei 177 von 984: testbtc-testusdt.csv\n",
      "Verarbeite Datei 178 von 984: testaptf0testusdtf0.csv\n",
      "Verarbeite Datei 179 von 984: ctkust.csv\n",
      "Verarbeite Datei 180 von 984: testalgo_testusd.csv\n",
      "Verarbeite Datei 181 von 984: wminima_ust.csv\n",
      "Verarbeite Datei 182 von 984: omgf0ustf0.csv\n",
      "Verarbeite Datei 183 von 984: prmxusd.csv\n",
      "Verarbeite Datei 184 von 984: egldust.csv\n",
      "Verarbeite Datei 185 von 984: yggusd.csv\n",
      "Verarbeite Datei 186 von 984: batbtc.csv\n",
      "Verarbeite Datei 187 von 984: albt-usd.csv\n",
      "Verarbeite Datei 188 von 984: solf0_ustf0.csv\n",
      "Verarbeite Datei 189 von 984: celust.csv\n",
      "Verarbeite Datei 190 von 984: flrusd.csv\n",
      "Verarbeite Datei 191 von 984: luxousd.csv\n",
      "Verarbeite Datei 192 von 984: ocean_usd.csv\n",
      "Verarbeite Datei 193 von 984: eut_mxnt.csv\n",
      "Verarbeite Datei 194 von 984: aaabbb.csv\n",
      "Verarbeite Datei 195 von 984: reefusd.csv\n",
      "Verarbeite Datei 196 von 984: uosusd.csv\n",
      "Verarbeite Datei 197 von 984: neoust.csv\n",
      "Verarbeite Datei 198 von 984: bttusd.csv\n",
      "Verarbeite Datei 199 von 984: uosbtc.csv\n",
      "Verarbeite Datei 200 von 984: testbtcf0-testusdtf0.csv\n",
      "Verarbeite Datei 201 von 984: oceanusd.csv\n",
      "Verarbeite Datei 202 von 984: fbtust.csv\n",
      "Verarbeite Datei 203 von 984: spell_usd.csv\n",
      "Verarbeite Datei 204 von 984: avaxusd.csv\n",
      "Verarbeite Datei 205 von 984: avax-ust.csv\n",
      "Verarbeite Datei 206 von 984: btcf0_eutf0.csv\n",
      "Verarbeite Datei 207 von 984: sushi_usd.csv\n",
      "Verarbeite Datei 208 von 984: karate_usd.csv\n",
      "Verarbeite Datei 209 von 984: forth-usd.csv\n",
      "Verarbeite Datei 210 von 984: anteth.csv\n",
      "Verarbeite Datei 211 von 984: link_ust.csv\n",
      "Verarbeite Datei 212 von 984: wbteth.csv\n",
      "Verarbeite Datei 213 von 984: mirust.csv\n",
      "Verarbeite Datei 214 von 984: near_ust.csv\n",
      "Verarbeite Datei 215 von 984: linkusd.csv\n",
      "Verarbeite Datei 216 von 984: testltcf0testusdtf0.csv\n",
      "Verarbeite Datei 217 von 984: xaut_btc.csv\n",
      "Verarbeite Datei 218 von 984: testxauttestusd.csv\n",
      "Verarbeite Datei 219 von 984: eurf0_ustf0.csv\n",
      "Verarbeite Datei 220 von 984: wncgusd.csv\n",
      "Verarbeite Datei 221 von 984: best_usd.csv\n",
      "Verarbeite Datei 222 von 984: shftusd.csv\n",
      "Verarbeite Datei 223 von 984: whbt_usd.csv\n",
      "Verarbeite Datei 224 von 984: egldf0ustf0.csv\n",
      "Verarbeite Datei 225 von 984: atoeth.csv\n",
      "Verarbeite Datei 226 von 984: testmatic_testusd.csv\n",
      "Verarbeite Datei 227 von 984: neobtc.csv\n",
      "Verarbeite Datei 228 von 984: ltcf0_ustf0.csv\n",
      "Verarbeite Datei 229 von 984: polc_usd.csv\n",
      "Verarbeite Datei 230 von 984: doge_ust.csv\n",
      "Verarbeite Datei 231 von 984: testfiltestusd.csv\n",
      "Verarbeite Datei 232 von 984: 1inch-usd.csv\n",
      "Verarbeite Datei 233 von 984: ethw_usd.csv\n",
      "Verarbeite Datei 234 von 984: nexo-usd.csv\n",
      "Verarbeite Datei 235 von 984: ampusd.csv\n",
      "Verarbeite Datei 236 von 984: sushi_ust.csv\n",
      "Verarbeite Datei 237 von 984: nomust.csv\n",
      "Verarbeite Datei 238 von 984: linkf0_ustf0.csv\n",
      "Verarbeite Datei 239 von 984: egld_usd.csv\n",
      "Verarbeite Datei 240 von 984: 1inch_usd.csv\n",
      "Verarbeite Datei 241 von 984: tonusd.csv\n",
      "Verarbeite Datei 242 von 984: xtzusd.csv\n",
      "Verarbeite Datei 243 von 984: ocean-ust.csv\n",
      "Verarbeite Datei 244 von 984: ampf0ustf0.csv\n",
      "Verarbeite Datei 245 von 984: solbtc.csv\n",
      "Verarbeite Datei 246 von 984: testada_testusd.csv\n",
      "Verarbeite Datei 247 von 984: nexo-btc.csv\n",
      "Verarbeite Datei 248 von 984: waves_ust.csv\n",
      "Verarbeite Datei 249 von 984: compusd.csv\n",
      "Verarbeite Datei 250 von 984: qrdoust.csv\n",
      "Verarbeite Datei 251 von 984: celusd.csv\n",
      "Verarbeite Datei 252 von 984: unif0ustf0.csv\n",
      "Verarbeite Datei 253 von 984: srmusd.csv\n",
      "Verarbeite Datei 254 von 984: europe50ixf0_ustf0.csv\n",
      "Verarbeite Datei 255 von 984: nxrausd.csv\n",
      "Verarbeite Datei 256 von 984: hotusd.csv\n",
      "Verarbeite Datei 257 von 984: dusk-btc.csv\n",
      "Verarbeite Datei 258 von 984: sukuusd.csv\n",
      "Verarbeite Datei 259 von 984: hongkong50ixf0_ustf0.csv\n",
      "Verarbeite Datei 260 von 984: sntusd.csv\n",
      "Verarbeite Datei 261 von 984: ethf0ustf0.csv\n",
      "Verarbeite Datei 262 von 984: bandusd.csv\n",
      "Verarbeite Datei 263 von 984: eth2xust.csv\n",
      "Verarbeite Datei 264 von 984: neoeth.csv\n",
      "Verarbeite Datei 265 von 984: btcmim.csv\n",
      "Verarbeite Datei 266 von 984: treebusd.csv\n",
      "Verarbeite Datei 267 von 984: testfilf0testusdtf0.csv\n",
      "Verarbeite Datei 268 von 984: nexo_ust.csv\n",
      "Verarbeite Datei 269 von 984: kanusd.csv\n",
      "Verarbeite Datei 270 von 984: kaiust.csv\n",
      "Verarbeite Datei 271 von 984: unif0_ustf0.csv\n",
      "Verarbeite Datei 272 von 984: kncf0_ustf0.csv\n",
      "Verarbeite Datei 273 von 984: near-ust.csv\n",
      "Verarbeite Datei 274 von 984: testeostestusd.csv\n",
      "Verarbeite Datei 275 von 984: link_usd.csv\n",
      "Verarbeite Datei 276 von 984: solf0_btcf0.csv\n",
      "Verarbeite Datei 277 von 984: dotusd.csv\n",
      "Verarbeite Datei 278 von 984: smrust.csv\n",
      "Verarbeite Datei 279 von 984: australia200ixf0ustf0.csv\n",
      "Verarbeite Datei 280 von 984: iotf0ustf0.csv\n",
      "Verarbeite Datei 281 von 984: xvgusd.csv\n",
      "Verarbeite Datei 282 von 984: btcxch.csv\n",
      "Verarbeite Datei 283 von 984: zrxeth.csv\n",
      "Verarbeite Datei 284 von 984: fttust.csv\n",
      "Verarbeite Datei 285 von 984: maticusd.csv\n",
      "Verarbeite Datei 286 von 984: nearust.csv\n",
      "Verarbeite Datei 287 von 984: xautf0_ustf0.csv\n",
      "Verarbeite Datei 288 von 984: blur_ust.csv\n",
      "Verarbeite Datei 289 von 984: aaveust.csv\n",
      "Verarbeite Datei 290 von 984: blurust.csv\n",
      "Verarbeite Datei 291 von 984: omnbtc.csv\n",
      "Verarbeite Datei 292 von 984: dogef0_ustf0.csv\n",
      "Verarbeite Datei 293 von 984: testdottestusd.csv\n",
      "Verarbeite Datei 294 von 984: wild_usd.csv\n",
      "Verarbeite Datei 295 von 984: iosusd.csv\n",
      "Verarbeite Datei 296 von 984: gptusd.csv\n",
      "Verarbeite Datei 297 von 984: btceur.csv\n",
      "Verarbeite Datei 298 von 984: btcdomf0-ustf0.csv\n",
      "Verarbeite Datei 299 von 984: wild_ust.csv\n",
      "Verarbeite Datei 300 von 984: compf0_ustf0.csv\n",
      "Verarbeite Datei 301 von 984: dotust.csv\n",
      "Verarbeite Datei 302 von 984: eosf0-ustf0.csv\n",
      "Verarbeite Datei 303 von 984: waves-ust.csv\n",
      "Verarbeite Datei 304 von 984: sandf0ustf0.csv\n",
      "Verarbeite Datei 305 von 984: wbtusd.csv\n",
      "Verarbeite Datei 306 von 984: dvfusd.csv\n",
      "Verarbeite Datei 307 von 984: testxtzf0testusdtf0.csv\n",
      "Verarbeite Datei 308 von 984: udcusd.csv\n",
      "Verarbeite Datei 309 von 984: bandust.csv\n",
      "Verarbeite Datei 310 von 984: boson-usd.csv\n",
      "Verarbeite Datei 311 von 984: compf0ustf0.csv\n",
      "Verarbeite Datei 312 von 984: sanbtc.csv\n",
      "Verarbeite Datei 313 von 984: sgbusd.csv\n",
      "Verarbeite Datei 314 von 984: sanusd.csv\n",
      "Verarbeite Datei 315 von 984: ltcf0btcf0.csv\n",
      "Verarbeite Datei 316 von 984: floki_ust.csv\n",
      "Verarbeite Datei 317 von 984: btgusd.csv\n",
      "Verarbeite Datei 318 von 984: paxusd.csv\n",
      "Verarbeite Datei 319 von 984: xlmf0-ustf0.csv\n",
      "Verarbeite Datei 320 von 984: ldoust.csv\n",
      "Verarbeite Datei 321 von 984: avaxf0btcf0.csv\n",
      "Verarbeite Datei 322 von 984: sidususd.csv\n",
      "Verarbeite Datei 323 von 984: sushi-usd.csv\n",
      "Verarbeite Datei 324 von 984: testsol_testusd.csv\n",
      "Verarbeite Datei 325 von 984: nearf0ustf0.csv\n",
      "Verarbeite Datei 326 von 984: sngusd.csv\n",
      "Verarbeite Datei 327 von 984: mlnusd.csv\n",
      "Verarbeite Datei 328 von 984: testsolf0_testusdtf0.csv\n",
      "Verarbeite Datei 329 von 984: cnhcnht.csv\n",
      "Verarbeite Datei 330 von 984: xcadusd.csv\n",
      "Verarbeite Datei 331 von 984: etcust.csv\n",
      "Verarbeite Datei 332 von 984: shibusd.csv\n",
      "Verarbeite Datei 333 von 984: pepe_ust.csv\n",
      "Verarbeite Datei 334 von 984: btcgbp.csv\n",
      "Verarbeite Datei 335 von 984: qrdousd.csv\n",
      "Verarbeite Datei 336 von 984: xaut_usd.csv\n",
      "Verarbeite Datei 337 von 984: aave_usd.csv\n",
      "Verarbeite Datei 338 von 984: gnousd.csv\n",
      "Verarbeite Datei 339 von 984: error.csv\n",
      "Verarbeite Datei 340 von 984: wooust.csv\n",
      "Verarbeite Datei 341 von 984: leoeos.csv\n",
      "Verarbeite Datei 342 von 984: apeust.csv\n",
      "Verarbeite Datei 343 von 984: adausd.csv\n",
      "Verarbeite Datei 344 von 984: btctry.csv\n",
      "Verarbeite Datei 345 von 984: clousd.csv\n",
      "Verarbeite Datei 346 von 984: sgbust.csv\n",
      "Verarbeite Datei 347 von 984: btcf0-ustf0.csv\n",
      "Verarbeite Datei 348 von 984: chsb_usd.csv\n",
      "Verarbeite Datei 349 von 984: pngusd.csv\n",
      "Verarbeite Datei 350 von 984: testdotf0_testusdtf0.csv\n",
      "Verarbeite Datei 351 von 984: nxra_usd.csv\n",
      "Verarbeite Datei 352 von 984: testxautf0testusdtf0.csv\n",
      "Verarbeite Datei 353 von 984: xautust.csv\n",
      "Verarbeite Datei 354 von 984: eosbtc.csv\n",
      "Verarbeite Datei 355 von 984: maticust.csv\n",
      "Verarbeite Datei 356 von 984: avtusd.csv\n",
      "Verarbeite Datei 357 von 984: dotbtc.csv\n",
      "Verarbeite Datei 358 von 984: bmnbtc.csv\n",
      "Verarbeite Datei 359 von 984: apeusd.csv\n",
      "Verarbeite Datei 360 von 984: oneust.csv\n",
      "Verarbeite Datei 361 von 984: kanust.csv\n",
      "Verarbeite Datei 362 von 984: eth2xusd.csv\n",
      "Verarbeite Datei 363 von 984: testmaticf0testusdtf0.csv\n",
      "Verarbeite Datei 364 von 984: oxyust.csv\n",
      "Verarbeite Datei 365 von 984: ust_mxnt.csv\n",
      "Verarbeite Datei 366 von 984: balust.csv\n",
      "Verarbeite Datei 367 von 984: nexo_btc.csv\n",
      "Verarbeite Datei 368 von 984: tsdusd.csv\n",
      "Verarbeite Datei 369 von 984: solust.csv\n",
      "Verarbeite Datei 370 von 984: eosf0_ustf0.csv\n",
      "Verarbeite Datei 371 von 984: solf0ustf0.csv\n",
      "Verarbeite Datei 372 von 984: chexusd.csv\n",
      "Verarbeite Datei 373 von 984: sushif0ustf0.csv\n",
      "Verarbeite Datei 374 von 984: aixust.csv\n",
      "Verarbeite Datei 375 von 984: iotf0_ustf0.csv\n",
      "Verarbeite Datei 376 von 984: atof0ustf0.csv\n",
      "Verarbeite Datei 377 von 984: daibtc.csv\n",
      "Verarbeite Datei 378 von 984: dotf0_ustf0.csv\n",
      "Verarbeite Datei 379 von 984: gbpf0_ustf0.csv\n",
      "Verarbeite Datei 380 von 984: gxtusd.csv\n",
      "Verarbeite Datei 381 von 984: atlasusd.csv\n",
      "Verarbeite Datei 382 von 984: mimusd.csv\n",
      "Verarbeite Datei 383 von 984: atlas_usd.csv\n",
      "Verarbeite Datei 384 von 984: apenftusd.csv\n",
      "Verarbeite Datei 385 von 984: saneth.csv\n",
      "Verarbeite Datei 386 von 984: forth_usd.csv\n",
      "Verarbeite Datei 387 von 984: funusd.csv\n",
      "Verarbeite Datei 388 von 984: atlas_ust.csv\n",
      "Verarbeite Datei 389 von 984: btse_usd.csv\n",
      "Verarbeite Datei 390 von 984: xrpust.csv\n",
      "Verarbeite Datei 391 von 984: dogef0-ustf0.csv\n",
      "Verarbeite Datei 392 von 984: tryust.csv\n",
      "Verarbeite Datei 393 von 984: xmrusd.csv\n",
      "Verarbeite Datei 394 von 984: gtxust.csv\n",
      "Verarbeite Datei 395 von 984: daiusd.csv\n",
      "Verarbeite Datei 396 von 984: theta_ust.csv\n",
      "Verarbeite Datei 397 von 984: bchn-usd.csv\n",
      "Verarbeite Datei 398 von 984: vrausd.csv\n",
      "Verarbeite Datei 399 von 984: testapt_testusd.csv\n",
      "Verarbeite Datei 400 von 984: trxeth.csv\n",
      "Verarbeite Datei 401 von 984: pluusd.csv\n",
      "Verarbeite Datei 402 von 984: gptust.csv\n",
      "Verarbeite Datei 403 von 984: okbusd.csv\n",
      "Verarbeite Datei 404 von 984: egldusd.csv\n",
      "Verarbeite Datei 405 von 984: zecbtc.csv\n",
      "Verarbeite Datei 406 von 984: algusd.csv\n",
      "Verarbeite Datei 407 von 984: adabtc.csv\n",
      "Verarbeite Datei 408 von 984: egldf0_ustf0.csv\n",
      "Verarbeite Datei 409 von 984: velousd.csv\n",
      "Verarbeite Datei 410 von 984: orsusd.csv\n",
      "Verarbeite Datei 411 von 984: yfiust.csv\n",
      "Verarbeite Datei 412 von 984: egld_ust.csv\n",
      "Verarbeite Datei 413 von 984: etpeth.csv\n",
      "Verarbeite Datei 414 von 984: iotusd.csv\n",
      "Verarbeite Datei 415 von 984: france40ixf0_ustf0.csv\n",
      "Verarbeite Datei 416 von 984: best-usd.csv\n",
      "Verarbeite Datei 417 von 984: icpust.csv\n",
      "Verarbeite Datei 418 von 984: vetust.csv\n",
      "Verarbeite Datei 419 von 984: wminimausd.csv\n",
      "Verarbeite Datei 420 von 984: whbt_ust.csv\n",
      "Verarbeite Datei 421 von 984: chsbust.csv\n",
      "Verarbeite Datei 422 von 984: edobtc.csv\n",
      "Verarbeite Datei 423 von 984: b2must.csv\n",
      "Verarbeite Datei 424 von 984: egld-usd.csv\n",
      "Verarbeite Datei 425 von 984: uniust.csv\n",
      "Verarbeite Datei 426 von 984: nexobtc.csv\n",
      "Verarbeite Datei 427 von 984: doge-usd.csv\n",
      "Verarbeite Datei 428 von 984: nexo_usd.csv\n",
      "Verarbeite Datei 429 von 984: stgust.csv\n",
      "Verarbeite Datei 430 von 984: drnusd.csv\n",
      "Verarbeite Datei 431 von 984: ustusd.csv\n",
      "Verarbeite Datei 432 von 984: zrxbtc.csv\n",
      "Verarbeite Datei 433 von 984: bmiusd.csv\n",
      "Verarbeite Datei 434 von 984: testalgof0testusdtf0.csv\n",
      "Verarbeite Datei 435 von 984: btcdomf0_ustf0.csv\n",
      "Verarbeite Datei 436 von 984: oceanust.csv\n",
      "Verarbeite Datei 437 von 984: chsb-btc.csv\n",
      "Verarbeite Datei 438 von 984: xptf0_ustf0.csv\n",
      "Verarbeite Datei 439 von 984: mtnusd.csv\n",
      "Verarbeite Datei 440 von 984: avaxf0_ustf0.csv\n",
      "Verarbeite Datei 441 von 984: eth2x_ust.csv\n",
      "Verarbeite Datei 442 von 984: dcrusd.csv\n",
      "Verarbeite Datei 443 von 984: testaptf0_testusdtf0.csv\n",
      "Verarbeite Datei 444 von 984: trade_usd.csv\n",
      "Verarbeite Datei 445 von 984: chzust.csv\n",
      "Verarbeite Datei 446 von 984: leoust.csv\n",
      "Verarbeite Datei 447 von 984: mxnt_usd.csv\n",
      "Verarbeite Datei 448 von 984: france40ixf0ustf0.csv\n",
      "Verarbeite Datei 449 von 984: xtzf0ustf0.csv\n",
      "Verarbeite Datei 450 von 984: fttusd.csv\n",
      "Verarbeite Datei 451 von 984: neof0_ustf0.csv\n",
      "Verarbeite Datei 452 von 984: 1inchusd.csv\n",
      "Verarbeite Datei 453 von 984: qtmbtc.csv\n",
      "Verarbeite Datei 454 von 984: mkrust.csv\n",
      "Verarbeite Datei 455 von 984: jasmyf0ustf0.csv\n",
      "Verarbeite Datei 456 von 984: sunusd.csv\n",
      "Verarbeite Datei 457 von 984: paxust.csv\n",
      "Verarbeite Datei 458 von 984: testmaticf0_testusdtf0.csv\n",
      "Verarbeite Datei 459 von 984: treebust.csv\n",
      "Verarbeite Datei 460 von 984: eosdt-usd.csv\n",
      "Verarbeite Datei 461 von 984: sukuust.csv\n",
      "Verarbeite Datei 462 von 984: thetaust.csv\n",
      "Verarbeite Datei 463 von 984: aixusd.csv\n",
      "Verarbeite Datei 464 von 984: b2musd.csv\n",
      "Verarbeite Datei 465 von 984: iotbtc.csv\n",
      "Verarbeite Datei 466 von 984: solf0-ustf0.csv\n",
      "Verarbeite Datei 467 von 984: sxxusd.csv\n",
      "Verarbeite Datei 468 von 984: iotgbp.csv\n",
      "Verarbeite Datei 469 von 984: axsf0ustf0.csv\n",
      "Verarbeite Datei 470 von 984: repusd.csv\n",
      "Verarbeite Datei 471 von 984: bchnusd.csv\n",
      "Verarbeite Datei 472 von 984: blur_usd.csv\n",
      "Verarbeite Datei 473 von 984: nomusd.csv\n",
      "Verarbeite Datei 474 von 984: ltcf0-btcf0.csv\n",
      "Verarbeite Datei 475 von 984: suiusd.csv\n",
      "Verarbeite Datei 476 von 984: omgf0_ustf0.csv\n",
      "Verarbeite Datei 477 von 984: suku-usd.csv\n",
      "Verarbeite Datei 478 von 984: atoust.csv\n",
      "Verarbeite Datei 479 von 984: atousd.csv\n",
      "Verarbeite Datei 480 von 984: doge_usd.csv\n",
      "Verarbeite Datei 481 von 984: crvust.csv\n",
      "Verarbeite Datei 482 von 984: rcnusd.csv\n",
      "Verarbeite Datei 483 von 984: testdogetestusd.csv\n",
      "Verarbeite Datei 484 von 984: luna2ust.csv\n",
      "Verarbeite Datei 485 von 984: ethf0_eutf0.csv\n",
      "Verarbeite Datei 486 von 984: xptf0ustf0.csv\n",
      "Verarbeite Datei 487 von 984: suku_usd.csv\n",
      "Verarbeite Datei 488 von 984: stgf0_ustf0.csv\n",
      "Verarbeite Datei 489 von 984: jstusd.csv\n",
      "Verarbeite Datei 490 von 984: mgousd.csv\n",
      "Verarbeite Datei 491 von 984: qtfbtc.csv\n",
      "Verarbeite Datei 492 von 984: balusd.csv\n",
      "Verarbeite Datei 493 von 984: hecust.csv\n",
      "Verarbeite Datei 494 von 984: maticbtc.csv\n",
      "Verarbeite Datei 495 von 984: eth2x-eth.csv\n",
      "Verarbeite Datei 496 von 984: ccdust.csv\n",
      "Verarbeite Datei 497 von 984: testeth_testusd.csv\n",
      "Verarbeite Datei 498 von 984: wbtbtc.csv\n",
      "Verarbeite Datei 499 von 984: xtzf0_ustf0.csv\n",
      "Verarbeite Datei 500 von 984: sand_ust.csv\n",
      "Verarbeite Datei 501 von 984: batust.csv\n",
      "Verarbeite Datei 502 von 984: linkf0ustf0.csv\n",
      "Verarbeite Datei 503 von 984: luna_usd.csv\n",
      "Verarbeite Datei 504 von 984: testadaf0testusdtf0.csv\n",
      "Verarbeite Datei 505 von 984: xrdbtc.csv\n",
      "Verarbeite Datei 506 von 984: flokiusd.csv\n",
      "Verarbeite Datei 507 von 984: apef0_ustf0.csv\n",
      "Verarbeite Datei 508 von 984: arbusd.csv\n",
      "Verarbeite Datei 509 von 984: necusd.csv\n",
      "Verarbeite Datei 510 von 984: rbtbtc.csv\n",
      "Verarbeite Datei 511 von 984: btc_mxnt.csv\n",
      "Verarbeite Datei 512 von 984: fetust.csv\n",
      "Verarbeite Datei 513 von 984: bmiust.csv\n",
      "Verarbeite Datei 514 von 984: shib_ust.csv\n",
      "Verarbeite Datei 515 von 984: iotf0-ustf0.csv\n",
      "Verarbeite Datei 516 von 984: btc_xaut.csv\n",
      "Verarbeite Datei 517 von 984: tradeusd.csv\n",
      "Verarbeite Datei 518 von 984: eth2x_usd.csv\n",
      "Verarbeite Datei 519 von 984: shibf0ustf0.csv\n",
      "Verarbeite Datei 520 von 984: b21x-usd.csv\n",
      "Verarbeite Datei 521 von 984: xautf0_btcf0.csv\n",
      "Verarbeite Datei 522 von 984: wtcusd.csv\n",
      "Verarbeite Datei 523 von 984: iqxust.csv\n",
      "Verarbeite Datei 524 von 984: rlyust.csv\n",
      "Verarbeite Datei 525 von 984: matic_btc.csv\n",
      "Verarbeite Datei 526 von 984: testbtc-testusd.csv\n",
      "Verarbeite Datei 527 von 984: etpusd.csv\n",
      "Verarbeite Datei 528 von 984: suku_ust.csv\n",
      "Verarbeite Datei 529 von 984: eususd.csv\n",
      "Verarbeite Datei 530 von 984: nutusd.csv\n",
      "Verarbeite Datei 531 von 984: ctxusd.csv\n",
      "Verarbeite Datei 532 von 984: linkf0-ustf0.csv\n",
      "Verarbeite Datei 533 von 984: iotjpy.csv\n",
      "Verarbeite Datei 534 von 984: udcust.csv\n",
      "Verarbeite Datei 535 von 984: luxo_usd.csv\n",
      "Verarbeite Datei 536 von 984: eosf0ustf0.csv\n",
      "Verarbeite Datei 537 von 984: jasmy_usd.csv\n",
      "Verarbeite Datei 538 von 984: sushif0_ustf0.csv\n",
      "Verarbeite Datei 539 von 984: chzusd.csv\n",
      "Verarbeite Datei 540 von 984: solf0btcf0.csv\n",
      "Verarbeite Datei 541 von 984: qtfusd.csv\n",
      "Verarbeite Datei 542 von 984: opxusd.csv\n",
      "Verarbeite Datei 543 von 984: testavaxtestusd.csv\n",
      "Verarbeite Datei 544 von 984: testethtestusd.csv\n",
      "Verarbeite Datei 545 von 984: eoseur.csv\n",
      "Verarbeite Datei 546 von 984: brise_usd.csv\n",
      "Verarbeite Datei 547 von 984: mnabtc.csv\n",
      "Verarbeite Datei 548 von 984: testfil_testusd.csv\n",
      "Verarbeite Datei 549 von 984: zilusd.csv\n",
      "Verarbeite Datei 550 von 984: testbtcf0_testusdtf0.csv\n",
      "Verarbeite Datei 551 von 984: etheut.csv\n",
      "Verarbeite Datei 552 von 984: iceusd.csv\n",
      "Verarbeite Datei 553 von 984: eth2x_eth.csv\n",
      "Verarbeite Datei 554 von 984: enjusd.csv\n",
      "Verarbeite Datei 555 von 984: gala_ust.csv\n",
      "Verarbeite Datei 556 von 984: arbust.csv\n",
      "Verarbeite Datei 557 von 984: wminimaust.csv\n",
      "Verarbeite Datei 558 von 984: xchusd.csv\n",
      "Verarbeite Datei 559 von 984: btcjpy.csv\n",
      "Verarbeite Datei 560 von 984: poausd.csv\n",
      "Verarbeite Datei 561 von 984: xmrf0ustf0.csv\n",
      "Verarbeite Datei 562 von 984: ksmust.csv\n",
      "Verarbeite Datei 563 von 984: fclusd.csv\n",
      "Verarbeite Datei 564 von 984: uk100ixf0ustf0.csv\n",
      "Verarbeite Datei 565 von 984: ethust.csv\n",
      "Verarbeite Datei 566 von 984: sweat_ust.csv\n",
      "Verarbeite Datei 567 von 984: chex-usd.csv\n",
      "Verarbeite Datei 568 von 984: luna2_usd.csv\n",
      "Verarbeite Datei 569 von 984: neogbp.csv\n",
      "Verarbeite Datei 570 von 984: testbtctestusdt.csv\n",
      "Verarbeite Datei 571 von 984: bobaust.csv\n",
      "Verarbeite Datei 572 von 984: code.csv\n",
      "Verarbeite Datei 573 von 984: duskusd.csv\n",
      "Verarbeite Datei 574 von 984: antusd.csv\n",
      "Verarbeite Datei 575 von 984: pepe_usd.csv\n",
      "Verarbeite Datei 576 von 984: chsb-ust.csv\n",
      "Verarbeite Datei 577 von 984: xlmbtc.csv\n",
      "Verarbeite Datei 578 von 984: xpdf0ustf0.csv\n",
      "Verarbeite Datei 579 von 984: eth2x-usd.csv\n",
      "Verarbeite Datei 580 von 984: flrust.csv\n",
      "Verarbeite Datei 581 von 984: genusd.csv\n",
      "Verarbeite Datei 582 von 984: planets-usd.csv\n",
      "Verarbeite Datei 583 von 984: polis_ust.csv\n",
      "Verarbeite Datei 584 von 984: btcf0ustf0.csv\n",
      "Verarbeite Datei 585 von 984: eosust.csv\n",
      "Verarbeite Datei 586 von 984: hecusd.csv\n",
      "Verarbeite Datei 587 von 984: cndusd.csv\n",
      "Verarbeite Datei 588 von 984: apef0ustf0.csv\n",
      "Verarbeite Datei 589 von 984: zecf0_ustf0.csv\n",
      "Verarbeite Datei 590 von 984: bosonust.csv\n",
      "Verarbeite Datei 591 von 984: planetsusd.csv\n",
      "Verarbeite Datei 592 von 984: avaxf0ustf0.csv\n",
      "Verarbeite Datei 593 von 984: etheur.csv\n",
      "Verarbeite Datei 594 von 984: btse-usd.csv\n",
      "Verarbeite Datei 595 von 984: ethgbp.csv\n",
      "Verarbeite Datei 596 von 984: zrxusd.csv\n",
      "Verarbeite Datei 597 von 984: booust.csv\n",
      "Verarbeite Datei 598 von 984: briseusd.csv\n",
      "Verarbeite Datei 599 von 984: dogeust.csv\n",
      "Verarbeite Datei 600 von 984: zecf0ustf0.csv\n",
      "Verarbeite Datei 601 von 984: uk100ixf0_ustf0.csv\n",
      "Verarbeite Datei 602 von 984: hmtusd.csv\n",
      "Verarbeite Datei 603 von 984: velo_usd.csv\n",
      "Verarbeite Datei 604 von 984: uniusd.csv\n",
      "Verarbeite Datei 605 von 984: gala_usd.csv\n",
      "Verarbeite Datei 606 von 984: ethmxnt.csv\n",
      "Verarbeite Datei 607 von 984: tradeust.csv\n",
      "Verarbeite Datei 608 von 984: xrpusd.csv\n",
      "Verarbeite Datei 609 von 984: sushi-ust.csv\n",
      "Verarbeite Datei 610 von 984: xcnust.csv\n",
      "Verarbeite Datei 611 von 984: testnearf0_testusdtf0.csv\n",
      "Verarbeite Datei 612 von 984: icpf0ustf0.csv\n",
      "Verarbeite Datei 613 von 984: velo_ust.csv\n",
      "Verarbeite Datei 614 von 984: testmatictestusdt.csv\n",
      "Verarbeite Datei 615 von 984: filusd.csv\n",
      "Verarbeite Datei 616 von 984: prmx_ust.csv\n",
      "Verarbeite Datei 617 von 984: hixust.csv\n",
      "Verarbeite Datei 618 von 984: bchabc-usd.csv\n",
      "Verarbeite Datei 619 von 984: bntusd.csv\n",
      "Verarbeite Datei 620 von 984: btcf0eutf0.csv\n",
      "Verarbeite Datei 621 von 984: forthusd.csv\n",
      "Verarbeite Datei 622 von 984: xrpf0btcf0.csv\n",
      "Verarbeite Datei 623 von 984: maticf0ustf0.csv\n",
      "Verarbeite Datei 624 von 984: galaf0ustf0.csv\n",
      "Verarbeite Datei 625 von 984: germany30ixf0-ustf0.csv\n",
      "Verarbeite Datei 626 von 984: ltcf0_btcf0.csv\n",
      "Verarbeite Datei 627 von 984: btccnht.csv\n",
      "Verarbeite Datei 628 von 984: uopust.csv\n",
      "Verarbeite Datei 629 von 984: ioteth.csv\n",
      "Verarbeite Datei 630 von 984: testalgof0_testusdtf0.csv\n",
      "Verarbeite Datei 631 von 984: leousd.csv\n",
      "Verarbeite Datei 632 von 984: oneusd.csv\n",
      "Verarbeite Datei 633 von 984: tenet_usd.csv\n",
      "Verarbeite Datei 634 von 984: testbtctestusd.csv\n",
      "Verarbeite Datei 635 von 984: gstust.csv\n",
      "Verarbeite Datei 636 von 984: theta_usd.csv\n",
      "Verarbeite Datei 637 von 984: idxbtc.csv\n",
      "Verarbeite Datei 638 von 984: ethf0btcf0.csv\n",
      "Verarbeite Datei 639 von 984: ftmf0_ustf0.csv\n",
      "Verarbeite Datei 640 von 984: yfiusd.csv\n",
      "Verarbeite Datei 641 von 984: xaut-usd.csv\n",
      "Verarbeite Datei 642 von 984: linkust.csv\n",
      "Verarbeite Datei 643 von 984: lrcbtc.csv\n",
      "Verarbeite Datei 644 von 984: polis_usd.csv\n",
      "Verarbeite Datei 645 von 984: boson_ust.csv\n",
      "Verarbeite Datei 646 von 984: axsf0_ustf0.csv\n",
      "Verarbeite Datei 647 von 984: testxautf0_testusdtf0.csv\n",
      "Verarbeite Datei 648 von 984: chsb-usd.csv\n",
      "Verarbeite Datei 649 von 984: seiusd.csv\n",
      "Verarbeite Datei 650 von 984: btc_cnht.csv\n",
      "Verarbeite Datei 651 von 984: opxust.csv\n",
      "Verarbeite Datei 652 von 984: testsoltestusd.csv\n",
      "Verarbeite Datei 653 von 984: srmust.csv\n",
      "Verarbeite Datei 654 von 984: jpyf0ustf0.csv\n",
      "Verarbeite Datei 655 von 984: eosdt-ust.csv\n",
      "Verarbeite Datei 656 von 984: filust.csv\n",
      "Verarbeite Datei 657 von 984: bsvbtc.csv\n",
      "Verarbeite Datei 658 von 984: dapp-usd.csv\n",
      "Verarbeite Datei 659 von 984: xsnusd.csv\n",
      "Verarbeite Datei 660 von 984: testltcf0_testusdtf0.csv\n",
      "Verarbeite Datei 661 von 984: gocust.csv\n",
      "Verarbeite Datei 662 von 984: omgusd.csv\n",
      "Verarbeite Datei 663 von 984: omgbtc.csv\n",
      "Verarbeite Datei 664 von 984: btgbtc.csv\n",
      "Verarbeite Datei 665 von 984: dogust.csv\n",
      "Verarbeite Datei 666 von 984: ccdbtc.csv\n",
      "Verarbeite Datei 667 von 984: gstusd.csv\n",
      "Verarbeite Datei 668 von 984: gntbtc.csv\n",
      "Verarbeite Datei 669 von 984: btcmxnt.csv\n",
      "Verarbeite Datei 670 von 984: comp_ust.csv\n",
      "Verarbeite Datei 671 von 984: trxust.csv\n",
      "Verarbeite Datei 672 von 984: xcheth.csv\n",
      "Verarbeite Datei 673 von 984: testeosf0_testusdtf0.csv\n",
      "Verarbeite Datei 674 von 984: rlyusd.csv\n",
      "Verarbeite Datei 675 von 984: matic_ust.csv\n",
      "Verarbeite Datei 676 von 984: aave_ust.csv\n",
      "Verarbeite Datei 677 von 984: band_ust.csv\n",
      "Verarbeite Datei 678 von 984: trxbtc.csv\n",
      "Verarbeite Datei 679 von 984: atobtc.csv\n",
      "Verarbeite Datei 680 von 984: hmtust.csv\n",
      "Verarbeite Datei 681 von 984: eurf0-ustf0.csv\n",
      "Verarbeite Datei 682 von 984: ethwust.csv\n",
      "Verarbeite Datei 683 von 984: ampust.csv\n",
      "Verarbeite Datei 684 von 984: senateusd.csv\n",
      "Verarbeite Datei 685 von 984: ioteur.csv\n",
      "Verarbeite Datei 686 von 984: stgf0ustf0.csv\n",
      "Verarbeite Datei 687 von 984: xautf0-ustf0.csv\n",
      "Verarbeite Datei 688 von 984: galaf0_ustf0.csv\n",
      "Verarbeite Datei 689 von 984: tenet_ust.csv\n",
      "Verarbeite Datei 690 von 984: apenftust.csv\n",
      "Verarbeite Datei 691 von 984: pasusd.csv\n",
      "Verarbeite Datei 692 von 984: ampf0-ustf0.csv\n",
      "Verarbeite Datei 693 von 984: xagf0ustf0.csv\n",
      "Verarbeite Datei 694 von 984: link-usd.csv\n",
      "Verarbeite Datei 695 von 984: nexoust.csv\n",
      "Verarbeite Datei 696 von 984: testethf0_testusdtf0.csv\n",
      "Verarbeite Datei 697 von 984: htxusd.csv\n",
      "Verarbeite Datei 698 von 984: jpyf0-ustf0.csv\n",
      "Verarbeite Datei 699 von 984: tlosusd.csv\n",
      "Verarbeite Datei 700 von 984: germany40ixf0_ustf0.csv\n",
      "Verarbeite Datei 701 von 984: stgusd.csv\n",
      "Verarbeite Datei 702 von 984: eosgbp.csv\n",
      "Verarbeite Datei 703 von 984: jasmy_ust.csv\n",
      "Verarbeite Datei 704 von 984: algf0_ustf0.csv\n",
      "Verarbeite Datei 705 von 984: europe50ixf0-ustf0.csv\n",
      "Verarbeite Datei 706 von 984: ancusd.csv\n",
      "Verarbeite Datei 707 von 984: forth_ust.csv\n",
      "Verarbeite Datei 708 von 984: senate_usd.csv\n",
      "Verarbeite Datei 709 von 984: xrpf0_btcf0.csv\n",
      "Verarbeite Datei 710 von 984: spain35ixf0_ustf0.csv\n",
      "Verarbeite Datei 711 von 984: bchabc_usd.csv\n",
      "Verarbeite Datei 712 von 984: thetausd.csv\n",
      "Verarbeite Datei 713 von 984: shib_usd.csv\n",
      "Verarbeite Datei 714 von 984: neof0ustf0.csv\n",
      "Verarbeite Datei 715 von 984: ust-cnht.csv\n",
      "Verarbeite Datei 716 von 984: neojpy.csv\n",
      "Verarbeite Datei 717 von 984: polisust.csv\n",
      "Verarbeite Datei 718 von 984: germany40ixf0ustf0.csv\n",
      "Verarbeite Datei 719 von 984: dotf0ustf0.csv\n",
      "Verarbeite Datei 720 von 984: polisusd.csv\n",
      "Verarbeite Datei 721 von 984: mnausd.csv\n",
      "Verarbeite Datei 722 von 984: fbtusd.csv\n",
      "Verarbeite Datei 723 von 984: aptf0ustf0.csv\n",
      "Verarbeite Datei 724 von 984: testdogef0testusdtf0.csv\n",
      "Verarbeite Datei 725 von 984: gbpf0ustf0.csv\n",
      "Verarbeite Datei 726 von 984: leobtc.csv\n",
      "Verarbeite Datei 727 von 984: ltcbtc.csv\n",
      "Verarbeite Datei 728 von 984: btcxaut.csv\n",
      "Verarbeite Datei 729 von 984: datbtc.csv\n",
      "Verarbeite Datei 730 von 984: nexo-ust.csv\n",
      "Verarbeite Datei 731 von 984: xlmusd.csv\n",
      "Verarbeite Datei 732 von 984: matic_usd.csv\n",
      "Verarbeite Datei 733 von 984: terraustusd.csv\n",
      "Verarbeite Datei 734 von 984: datusd.csv\n",
      "Verarbeite Datei 735 von 984: ukoilf0_ustf0.csv\n",
      "Verarbeite Datei 736 von 984: gocusd.csv\n",
      "Verarbeite Datei 737 von 984: jstbtc.csv\n",
      "Verarbeite Datei 738 von 984: planets_usd.csv\n",
      "Verarbeite Datei 739 von 984: veeusd.csv\n",
      "Verarbeite Datei 740 von 984: zecusd.csv\n",
      "Verarbeite Datei 741 von 984: xrpf0ustf0.csv\n",
      "Verarbeite Datei 742 von 984: gmtust.csv\n",
      "Verarbeite Datei 743 von 984: eosjpy.csv\n",
      "Verarbeite Datei 744 von 984: waves-usd.csv\n",
      "Verarbeite Datei 745 von 984: testxtztestusd.csv\n",
      "Verarbeite Datei 746 von 984: ctkusd.csv\n",
      "Verarbeite Datei 747 von 984: antbtc.csv\n",
      "Verarbeite Datei 748 von 984: ftmusd.csv\n",
      "Verarbeite Datei 749 von 984: avaxf0_btcf0.csv\n",
      "Verarbeite Datei 750 von 984: chsbbtc.csv\n",
      "Verarbeite Datei 751 von 984: eth2xeth.csv\n",
      "Verarbeite Datei 752 von 984: testdotf0testusdtf0.csv\n",
      "Verarbeite Datei 753 von 984: pngust.csv\n",
      "Verarbeite Datei 754 von 984: planets_ust.csv\n",
      "Verarbeite Datei 755 von 984: hongkong50ixf0ustf0.csv\n",
      "Verarbeite Datei 756 von 984: algf0ustf0.csv\n",
      "Verarbeite Datei 757 von 984: testavaxf0_testusdtf0.csv\n",
      "Verarbeite Datei 758 von 984: sushif0-ustf0.csv\n",
      "Verarbeite Datei 759 von 984: galausd.csv\n",
      "Verarbeite Datei 760 von 984: sxxust.csv\n",
      "Verarbeite Datei 761 von 984: testdot_testusd.csv\n",
      "Verarbeite Datei 762 von 984: astusd.csv\n",
      "Verarbeite Datei 763 von 984: spain35ixf0ustf0.csv\n",
      "Verarbeite Datei 764 von 984: eutmxnt.csv\n",
      "Verarbeite Datei 765 von 984: egld-ust.csv\n",
      "Verarbeite Datei 766 von 984: gbpust.csv\n",
      "Verarbeite Datei 767 von 984: testadatestusd.csv\n",
      "Verarbeite Datei 768 von 984: 1inchust.csv\n",
      "Verarbeite Datei 769 von 984: sunust.csv\n",
      "Verarbeite Datei 770 von 984: dusk_btc.csv\n",
      "Verarbeite Datei 771 von 984: dusk_usd.csv\n",
      "Verarbeite Datei 772 von 984: testmatictestusd.csv\n",
      "Verarbeite Datei 773 von 984: ringx-usd.csv\n",
      "Verarbeite Datei 774 von 984: gtxusd.csv\n",
      "Verarbeite Datei 775 von 984: planets-ust.csv\n",
      "Verarbeite Datei 776 von 984: trxeur.csv\n",
      "Verarbeite Datei 777 von 984: aavef0_ustf0.csv\n",
      "Verarbeite Datei 778 von 984: tlos_usd.csv\n",
      "Verarbeite Datei 779 von 984: fclust.csv\n",
      "Verarbeite Datei 780 von 984: reef_usd.csv\n",
      "Verarbeite Datei 781 von 984: gnteth.csv\n",
      "Verarbeite Datei 782 von 984: gbpeut.csv\n",
      "Verarbeite Datei 783 von 984: ethxaut.csv\n",
      "Verarbeite Datei 784 von 984: manusd.csv\n",
      "Verarbeite Datei 785 von 984: nearf0_ustf0.csv\n",
      "Verarbeite Datei 786 von 984: wncg_usd.csv\n",
      "Verarbeite Datei 787 von 984: trxf0ustf0.csv\n",
      "Verarbeite Datei 788 von 984: ethf0_btcf0.csv\n",
      "Verarbeite Datei 789 von 984: laiusd.csv\n",
      "Verarbeite Datei 790 von 984: luna2_ust.csv\n",
      "Verarbeite Datei 791 von 984: australia200ixf0_ustf0.csv\n",
      "Verarbeite Datei 792 von 984: snxusd.csv\n",
      "Verarbeite Datei 793 von 984: ustmxnt.csv\n",
      "Verarbeite Datei 794 von 984: aptusd.csv\n",
      "Verarbeite Datei 795 von 984: band_usd.csv\n",
      "Verarbeite Datei 796 von 984: testxtzf0_testusdtf0.csv\n",
      "Verarbeite Datei 797 von 984: ognusd.csv\n",
      "Verarbeite Datei 798 von 984: xlmf0_ustf0.csv\n",
      "Verarbeite Datei 799 von 984: boba_usd.csv\n",
      "Verarbeite Datei 800 von 984: sushiust.csv\n",
      "Verarbeite Datei 801 von 984: eutusd.csv\n",
      "Verarbeite Datei 802 von 984: brise_ust.csv\n",
      "Verarbeite Datei 803 von 984: bgbusd.csv\n",
      "Verarbeite Datei 804 von 984: zmtusd.csv\n",
      "Verarbeite Datei 805 von 984: terraust-ust.csv\n",
      "Verarbeite Datei 806 von 984: xautusd.csv\n",
      "Verarbeite Datei 807 von 984: bftusd.csv\n",
      "Verarbeite Datei 808 von 984: reefust.csv\n",
      "Verarbeite Datei 809 von 984: waxusd.csv\n",
      "Verarbeite Datei 810 von 984: algbtc.csv\n",
      "Verarbeite Datei 811 von 984: ukoilf0ustf0.csv\n",
      "Verarbeite Datei 812 von 984: aavef0ustf0.csv\n",
      "Verarbeite Datei 813 von 984: icpusd.csv\n",
      "Verarbeite Datei 814 von 984: jasmyusd.csv\n",
      "Verarbeite Datei 815 von 984: dotf0_btcf0.csv\n",
      "Verarbeite Datei 816 von 984: eurf0ustf0.csv\n",
      "Verarbeite Datei 817 von 984: mkrbtc.csv\n",
      "Verarbeite Datei 818 von 984: treeb_usd.csv\n",
      "Verarbeite Datei 819 von 984: karate_ust.csv\n",
      "Verarbeite Datei 820 von 984: wavesf0ustf0.csv\n",
      "Verarbeite Datei 821 von 984: aptf0_ustf0.csv\n",
      "Verarbeite Datei 822 von 984: convusd.csv\n",
      "Verarbeite Datei 823 von 984: near-usd.csv\n",
      "Verarbeite Datei 824 von 984: jasmyust.csv\n",
      "Verarbeite Datei 825 von 984: eth_mxnt.csv\n",
      "Verarbeite Datei 826 von 984: xtzust.csv\n",
      "Verarbeite Datei 827 von 984: albt-ust.csv\n",
      "Verarbeite Datei 828 von 984: japan225ixf0ustf0.csv\n",
      "Verarbeite Datei 829 von 984: testneartestusd.csv\n",
      "Verarbeite Datei 830 von 984: grtusd.csv\n",
      "Verarbeite Datei 831 von 984: swmusd.csv\n",
      "Verarbeite Datei 832 von 984: smrusd.csv\n",
      "Verarbeite Datei 833 von 984: testavax_testusd.csv\n",
      "Verarbeite Datei 834 von 984: dgbusd.csv\n",
      "Verarbeite Datei 835 von 984: btc-cnht.csv\n",
      "Verarbeite Datei 836 von 984: exousd.csv\n",
      "Verarbeite Datei 837 von 984: comp-ust.csv\n",
      "Verarbeite Datei 838 von 984: ampbtc.csv\n",
      "Verarbeite Datei 839 von 984: nutust.csv\n",
      "Verarbeite Datei 840 von 984: avax_usd.csv\n",
      "Verarbeite Datei 841 von 984: eoseth.csv\n",
      "Verarbeite Datei 842 von 984: crvf0ustf0.csv\n",
      "Verarbeite Datei 843 von 984: uskusd.csv\n",
      "Verarbeite Datei 844 von 984: omgeth.csv\n",
      "Verarbeite Datei 845 von 984: adaf0_ustf0.csv\n",
      "Verarbeite Datei 846 von 984: testmatic_testusdt.csv\n",
      "Verarbeite Datei 847 von 984: testdoge_testusd.csv\n",
      "Verarbeite Datei 848 von 984: 1inch-ust.csv\n",
      "Verarbeite Datei 849 von 984: dtxusd.csv\n",
      "Verarbeite Datei 850 von 984: etcusd.csv\n",
      "Verarbeite Datei 851 von 984: shibf0_ustf0.csv\n",
      "Verarbeite Datei 852 von 984: filf0_ustf0.csv\n",
      "Verarbeite Datei 853 von 984: uopusd.csv\n",
      "Verarbeite Datei 854 von 984: sandusd.csv\n",
      "Verarbeite Datei 855 von 984: compust.csv\n",
      "Verarbeite Datei 856 von 984: axsusd.csv\n",
      "Verarbeite Datei 857 von 984: xautbtc.csv\n",
      "Verarbeite Datei 858 von 984: tsdust.csv\n",
      "Verarbeite Datei 859 von 984: aaveusd.csv\n",
      "Verarbeite Datei 860 von 984: solusd.csv\n",
      "Verarbeite Datei 861 von 984: kncusd.csv\n",
      "Verarbeite Datei 862 von 984: maticf0_ustf0.csv\n",
      "Verarbeite Datei 863 von 984: dapp-ust.csv\n",
      "Verarbeite Datei 864 von 984: filf0ustf0.csv\n",
      "Verarbeite Datei 865 von 984: batusd.csv\n",
      "Verarbeite Datei 866 von 984: ocean_ust.csv\n",
      "Verarbeite Datei 867 von 984: xautf0-btcf0.csv\n",
      "Verarbeite Datei 868 von 984: gotusd.csv\n",
      "Verarbeite Datei 869 von 984: snxust.csv\n",
      "Verarbeite Datei 870 von 984: eosusd.csv\n",
      "Verarbeite Datei 871 von 984: chsbusd.csv\n",
      "Verarbeite Datei 872 von 984: ancust.csv\n",
      "Verarbeite Datei 873 von 984: kaiusd.csv\n",
      "Verarbeite Datei 874 von 984: shftust.csv\n",
      "Verarbeite Datei 875 von 984: luna-usd.csv\n",
      "Verarbeite Datei 876 von 984: kncbtc.csv\n",
      "Verarbeite Datei 877 von 984: ethf0eutf0.csv\n",
      "Verarbeite Datei 878 von 984: edousd.csv\n",
      "Verarbeite Datei 879 von 984: xrdusd.csv\n",
      "Verarbeite Datei 880 von 984: goteur.csv\n",
      "Verarbeite Datei 881 von 984: euteur.csv\n",
      "Verarbeite Datei 882 von 984: rrbust.csv\n",
      "Verarbeite Datei 883 von 984: bchabcusd.csv\n",
      "Verarbeite Datei 884 von 984: xagf0-ustf0.csv\n",
      "Verarbeite Datei 885 von 984: mkreth.csv\n",
      "Verarbeite Datei 886 von 984: htxust.csv\n",
      "Verarbeite Datei 887 von 984: mirusd.csv\n",
      "Verarbeite Datei 888 von 984: sweat_usd.csv\n",
      "Verarbeite Datei 889 von 984: xmrust.csv\n",
      "Verarbeite Datei 890 von 984: kncf0ustf0.csv\n",
      "Verarbeite Datei 891 von 984: dogeusd.csv\n",
      "Verarbeite Datei 892 von 984: doraust.csv\n",
      "Verarbeite Datei 893 von 984: nexousd.csv\n",
      "Verarbeite Datei 894 von 984: vraust.csv\n",
      "Verarbeite Datei 895 von 984: omnusd.csv\n",
      "Verarbeite Datei 896 von 984: lrcusd.csv\n",
      "Verarbeite Datei 897 von 984: rbtusd.csv\n",
      "Verarbeite Datei 898 von 984: link-ust.csv\n",
      "Verarbeite Datei 899 von 984: floki_usd.csv\n",
      "Verarbeite Datei 900 von 984: woousd.csv\n",
      "Verarbeite Datei 901 von 984: trxusd.csv\n",
      "Verarbeite Datei 902 von 984: planetsust.csv\n",
      "Verarbeite Datei 903 von 984: boxusd.csv\n",
      "Verarbeite Datei 904 von 984: qrdo_usd.csv\n",
      "Verarbeite Datei 905 von 984: vetusd.csv\n",
      "Verarbeite Datei 906 von 984: ftmf0ustf0.csv\n",
      "Verarbeite Datei 907 von 984: tknusd.csv\n",
      "Verarbeite Datei 908 von 984: adaf0-ustf0.csv\n",
      "Verarbeite Datei 909 von 984: sidus_usd.csv\n",
      "Verarbeite Datei 910 von 984: shft_usd.csv\n",
      "Verarbeite Datei 911 von 984: near_usd.csv\n",
      "Verarbeite Datei 912 von 984: atof0_ustf0.csv\n",
      "Verarbeite Datei 913 von 984: requsd.csv\n",
      "Verarbeite Datei 914 von 984: xlmust.csv\n",
      "Verarbeite Datei 915 von 984: testnear_testusd.csv\n",
      "Verarbeite Datei 916 von 984: bobausd.csv\n",
      "Verarbeite Datei 917 von 984: mobusd.csv\n",
      "Verarbeite Datei 918 von 984: testadaf0_testusdtf0.csv\n",
      "Verarbeite Datei 919 von 984: lunaust.csv\n",
      "Verarbeite Datei 920 von 984: mkrf0ustf0.csv\n",
      "Verarbeite Datei 921 von 984: testbtc_testusdt.csv\n",
      "Verarbeite Datei 922 von 984: tonust.csv\n",
      "Verarbeite Datei 923 von 984: mobust.csv\n",
      "Verarbeite Datei 924 von 984: ocean-usd.csv\n",
      "Verarbeite Datei 925 von 984: bmnusd.csv\n",
      "Verarbeite Datei 926 von 984: lunaf0-ustf0.csv\n",
      "Verarbeite Datei 927 von 984: eth2x-ust.csv\n",
      "Verarbeite Datei 928 von 984: band-ust.csv\n",
      "Verarbeite Datei 929 von 984: testavaxf0testusdtf0.csv\n",
      "Verarbeite Datei 930 von 984: idxusd.csv\n",
      "Verarbeite Datei 931 von 984: testnearf0testusdtf0.csv\n",
      "Verarbeite Datei 932 von 984: testxtz_testusd.csv\n",
      "Verarbeite Datei 933 von 984: aave-usd.csv\n",
      "Verarbeite Datei 934 von 984: testxaut_testusd.csv\n",
      "Verarbeite Datei 935 von 984: eutust.csv\n",
      "Verarbeite Datei 936 von 984: sweatust.csv\n",
      "Verarbeite Datei 937 von 984: ethwusd.csv\n",
      "Verarbeite Datei 938 von 984: comp_usd.csv\n",
      "Verarbeite Datei 939 von 984: dogef0ustf0.csv\n",
      "Verarbeite Datei 940 von 984: qtmusd.csv\n",
      "Verarbeite Datei 941 von 984: ncausd.csv\n",
      "Verarbeite Datei 942 von 984: bosonusd.csv\n",
      "Verarbeite Datei 943 von 984: grtust.csv\n",
      "Verarbeite Datei 944 von 984: duskbtc.csv\n",
      "Verarbeite Datei 945 von 984: testbtc_testusd.csv\n",
      "Verarbeite Datei 946 von 984: dogebtc.csv\n",
      "Verarbeite Datei 947 von 984: testsolf0testusdtf0.csv\n",
      "Verarbeite Datei 948 von 984: doge_btc.csv\n",
      "Verarbeite Datei 949 von 984: nearusd.csv\n",
      "Verarbeite Datei 950 von 984: waves_usd.csv\n",
      "Verarbeite Datei 951 von 984: xcnusd.csv\n",
      "Verarbeite Datei 952 von 984: daieth.csv\n",
      "Verarbeite Datei 953 von 984: ltcf0-ustf0.csv\n",
      "Verarbeite Datei 954 von 984: atlasust.csv\n",
      "Verarbeite Datei 955 von 984: testeosf0testusdtf0.csv\n",
      "Verarbeite Datei 956 von 984: ethusd.csv\n",
      "Verarbeite Datei 957 von 984: prmxust.csv\n",
      "Verarbeite Datei 958 von 984: seiust.csv\n",
      "Verarbeite Datei 959 von 984: ust_cnht.csv\n",
      "Verarbeite Datei 960 von 984: bestusd.csv\n",
      "Verarbeite Datei 961 von 984: laiust.csv\n",
      "Verarbeite Datei 962 von 984: cnh-cnht.csv\n",
      "Verarbeite Datei 963 von 984: band-usd.csv\n",
      "Verarbeite Datei 964 von 984: doge-ust.csv\n",
      "Verarbeite Datei 965 von 984: forth-ust.csv\n",
      "Verarbeite Datei 966 von 984: luna-btc.csv\n",
      "Verarbeite Datei 967 von 984: odeusd.csv\n",
      "Verarbeite Datei 968 von 984: btcf0_ustf0.csv\n",
      "Verarbeite Datei 969 von 984: hixusd.csv\n",
      "Verarbeite Datei 970 von 984: xaut-ust.csv\n",
      "Verarbeite Datei 971 von 984: 1inch_ust.csv\n",
      "Verarbeite Datei 972 von 984: etcf0_ustf0.csv\n",
      "Verarbeite Datei 973 von 984: trxf0_ustf0.csv\n",
      "Verarbeite Datei 974 von 984: suiust.csv\n",
      "Verarbeite Datei 975 von 984: ccdusd.csv\n",
      "Verarbeite Datei 976 von 984: ethbtc.csv\n",
      "Verarbeite Datei 977 von 984: boson_usd.csv\n",
      "Verarbeite Datei 978 von 984: zmtust.csv\n",
      "Verarbeite Datei 979 von 984: wildust.csv\n",
      "Verarbeite Datei 980 von 984: vetbtc.csv\n",
      "Verarbeite Datei 981 von 984: algust.csv\n",
      "Verarbeite Datei 982 von 984: fetusd.csv\n",
      "Verarbeite Datei 983 von 984: rifusd.csv\n",
      "Verarbeite Datei 984 von 984: ftmust.csv\n",
      "CSV-Datei wurde erfolgreich unter /home/ageq/Git_Projects/MLdatalake/source/combined.csv gespeichert.\n"
     ]
    }
   ],
   "source": [
    "# wandelt die Rohdaten in eine einzige CSV-Datei um\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def convert_csv_files(extract_to_path, output_csv_path):\n",
    "    cols = ['time', 'open', 'high', 'low', 'close', 'volume']\n",
    "    first_file = True\n",
    "    file_count = 0\n",
    "    \n",
    "    # Zählen der CSV-Dateien im Verzeichnis\n",
    "    total_files = sum([len(files) for r, d, files in os.walk(extract_to_path) if any(f.endswith('.csv') for f in files)])\n",
    "    \n",
    "    for root, dirs, files in os.walk(extract_to_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.csv'):\n",
    "                file_count += 1\n",
    "                print(f\"Verarbeite Datei {file_count} von {total_files}: {file}\")\n",
    "                #TODO - Dateigröße und zeitliche Dauer der Verarbeitung berechnen + Animation\n",
    "                file_path = os.path.join(root, file)\n",
    "                df = pd.read_csv(file_path, usecols=cols)\n",
    "                df['ticker'] = file.split('.')[0]\n",
    "                df = df[df['ticker'].str.contains('usd')]\n",
    "                df['date'] = pd.to_datetime(df['time'], unit='ms')\n",
    "                df = df.sort_values(by=['date', 'ticker']).drop(columns='time').set_index(['date', 'ticker'])\n",
    "                \n",
    "                if first_file:\n",
    "                    df.to_csv(output_csv_path, mode='w', header=True)\n",
    "                    first_file = False\n",
    "                else:\n",
    "                    df.to_csv(output_csv_path, mode='a', header=False)\n",
    "    \n",
    "    print(f\"CSV-Datei wurde erfolgreich unter {output_csv_path} gespeichert.\")\n",
    "\n",
    "def main_convert_csv():\n",
    "    try:\n",
    "        # Pfad zur config.json Datei\n",
    "        config_path = 'config.json'\n",
    "        \n",
    "        # Einlesen der Konfigurationsdatei\n",
    "        config = load_config(config_path)\n",
    "        \n",
    "        # Pfad zur Registry-Datei\n",
    "        source_path = '/home/ageq/Git_Projects/MLdatalake/source'\n",
    "        \n",
    "        # Erstellen des vollständigen Pfades zum Verzeichnis 'archive'\n",
    "        archive_path = os.path.join(source_path, 'archive')\n",
    "        output_csv_path = os.path.join(source_path, 'combined.csv')\n",
    "        \n",
    "        # Konvertieren der CSV-Dateien\n",
    "        convert_csv_files(archive_path, output_csv_path)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "main_convert_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importiert die Daten in die datalake Datenbank\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Pfad zur config.json Datei\n",
    "        config_path = 'config.json'\n",
    "        \n",
    "        # Einlesen der Konfigurationsdatei\n",
    "        config = load_config(config_path)\n",
    "        \n",
    "        # Pfad zur Registry-Datei\n",
    "        source_path = '/home/ageq/Git_Projects/MLdatalake/source'\n",
    "        \n",
    "        output_csv_path = os.path.join(source_path, \"combined.csv\")\n",
    "\n",
    "        \n",
    "        # Erstellen der Datenbank-Engine\n",
    "        engine = create_db_engine(config)\n",
    "        Base.metadata.create_all(engine)\n",
    "        Session = sessionmaker(bind=engine)\n",
    "        session = Session()\n",
    "\n",
    "        # Laden und Verarbeiten der Daten aus der CSV-Datei in Chunks\n",
    "        load_and_process_data_from_csv(output_csv_path, session)\n",
    "        \n",
    "        print(\"Data imported successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n",
      "pymysql ist bereits installiert.\n",
      "cryptography ist bereits installiert.\n",
      "pause\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgo\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 24\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     session \u001b[38;5;241m=\u001b[39m Session()\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# Laden und Verarbeiten der Daten aus der CSV-Datei in Chunks\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[43mload_and_process_data_from_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_csv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData imported successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m, in \u001b[0;36mload_and_process_data_from_csv\u001b[0;34m(csv_file_path, session, chunksize, symbol_filter)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_and_process_data_from_csv\u001b[39m(csv_file_path, session, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100000\u001b[39m, symbol_filter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Berechnen Sie die Gesamtzahl der Chunks\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpause\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m     total_rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcsv_file_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Minus 1 für die Header-Zeile\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     total_chunks \u001b[38;5;241m=\u001b[39m (total_rows \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m chunksize) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Initialisieren Sie den Zähler für die verarbeiteten Chunks\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_and_process_data_from_csv\u001b[39m(csv_file_path, session, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100000\u001b[39m, symbol_filter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Berechnen Sie die Gesamtzahl der Chunks\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpause\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m     total_rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(csv_file_path)) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Minus 1 für die Header-Zeile\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     total_chunks \u001b[38;5;241m=\u001b[39m (total_rows \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m chunksize) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Initialisieren Sie den Zähler für die verarbeiteten Chunks\u001b[39;00m\n",
      "File \u001b[0;32m~/Git_Projects/MLdatalake/venv/lib/python3.11/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_constants.py:566\u001b[0m, in \u001b[0;36mNO_FTRACE\u001b[0;34m(frame, event, arg)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    564\u001b[0m     _check_ftrace_set_none()\n\u001b[0;32m--> 566\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mNO_FTRACE\u001b[39m(frame, event, arg):\n\u001b[1;32m    567\u001b[0m         frame\u001b[38;5;241m.\u001b[39mf_trace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    568\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"go\")\n",
    "main()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##alternativer Zähler:\n",
    "# TODO - zu testen\n",
    "def process_and_insert_data(session, bars1m, symbol_filter=None):\n",
    "    if symbol_filter:\n",
    "        bars1m = bars1m.query(f'ticker == \"{symbol_filter}\"')\n",
    "    \n",
    "    # Resample auf 1-Minuten-Intervalle\n",
    "    bars1m = bars1m.reset_index().set_index('date').groupby('ticker').resample('1min').last().droplevel(0)\n",
    "    bars1m.loc[:, bars1m.columns[:-1]] = bars1m[bars1m.columns[:-1]].ffill()\n",
    "    bars1m.loc[:, 'volume'] = bars1m['volume'].fillna(value=0.0)\n",
    "    bars1m = bars1m.reset_index().sort_values(by=['date', 'ticker']).set_index(['date', 'ticker'])\n",
    "    \n",
    "    tickers = bars1m.index.get_level_values(1).unique()\n",
    "    latest_date = bars1m.index.get_level_values('date').max()\n",
    "    active_tickers = bars1m.loc[latest_date].index.get_level_values('ticker').unique()\n",
    "    \n",
    "    symbols = pd.DataFrame(tickers, columns=['ticker'])\n",
    "    symbols['name'] = symbols['ticker']\n",
    "    symbols['market'] = 'crypto'\n",
    "    symbols['active'] = np.where(symbols['ticker'].isin(active_tickers), True, False)\n",
    "    symbols = symbols.sort_values(by='ticker')\n",
    "    \n",
    "    total_symbols = len(symbols)\n",
    "    for i, r in enumerate(symbols.itertuples(), 1):\n",
    "        try:\n",
    "            print(f\"Uploading symbol {i}/{total_symbols}: {r.ticker}\")\n",
    "            \n",
    "            symbol = Symbol(ticker=r.ticker, name=r.name, market=Market[r.market], active=r.active)\n",
    "            session.add(symbol)\n",
    "            session.commit()\n",
    "            \n",
    "            # Überprüfen, ob der Index existiert\n",
    "            if r.ticker in bars1m.index.get_level_values('ticker'):\n",
    "                bars = bars1m.xs(r.ticker, level='ticker').reset_index()\n",
    "                bars['symbol_id'] = symbol.id\n",
    "                \n",
    "                session.bulk_insert_mappings(MinuteBar, bars.to_dict(orient='records'))\n",
    "                session.commit()\n",
    "            else:\n",
    "                print(f\"Ticker {r.ticker} nicht im Index gefunden.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while uploading symbol {r.ticker}: {e}\")\n",
    "            session.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, inspect, Column, Integer, String, Float, DateTime, Enum, ForeignKey, Boolean\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "url = \"mysql+pymysql://mldatalake_user:userpassword@localhost:3308/mldatalake\"\n",
    "engine = create_engine(url, echo=True)\n",
    "\n",
    "inspector = inspect(engine)\n",
    "table_names = inspector.get_table_names()\n",
    "print(table_names)\n",
    "\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, inspect, Column, Integer, String, Float, DateTime, Enum, ForeignKey, Boolean\n",
    "from sqlalchemy.orm import declarative_base, sessionmaker, relationship\n",
    "import enum\n",
    "\n",
    "# Datenbankverbindung herstellen\n",
    "url = \"mysql+pymysql://mldatalake_user:userpassword@localhost:3308/mldatalake\"\n",
    "engine = create_engine(url, echo=False)\n",
    "\n",
    "# Inspektor verwenden, um Tabellen zu überprüfen\n",
    "inspector = inspect(engine)\n",
    "table_names = inspector.get_table_names()\n",
    "print(\"Tabellen in der Datenbank:\", table_names)\n",
    "\n",
    "# Basis für SQLAlchemy-Modelle definieren\n",
    "Base = declarative_base()\n",
    "\n",
    "# Enum für den Markt\n",
    "class Market(enum.Enum):\n",
    "    crypto = 'crypto'\n",
    "    stock = 'stock'\n",
    "    forex = 'forex'\n",
    "    futures = 'futures'\n",
    "\n",
    "# Modell für die Tabelle 'symbol' definieren\n",
    "class Symbol(Base):\n",
    "    __tablename__ = 'symbol'\n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    ticker = Column(String(50), nullable=False)\n",
    "    name = Column(String(200), nullable=False)\n",
    "    market = Column(Enum(Market), nullable=False)\n",
    "    active = Column(Boolean, nullable=False)\n",
    "\n",
    "# Modell für die Tabelle 'minute_bar' definieren\n",
    "class MinuteBar(Base):\n",
    "    __tablename__ = 'minute_bar'\n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    date = Column(DateTime, nullable=False)\n",
    "    open = Column(Float)\n",
    "    high = Column(Float)\n",
    "    low = Column(Float)\n",
    "    close = Column(Float)\n",
    "    volume = Column(Float)\n",
    "    symbol_id = Column(Integer, ForeignKey('symbol.id', ondelete=\"CASCADE\"), nullable=False)\n",
    "    symbol = relationship('Symbol', backref='minute_bars')\n",
    "\n",
    "# Sitzung erstellen\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "# Überprüfen, ob die Tabelle 'minute_bar' existiert\n",
    "if 'minute_bar' in table_names:\n",
    "    # Anzahl der Einträge in der Tabelle 'minute_bar' überprüfen\n",
    "    count = session.query(MinuteBar).count()\n",
    "    print(f\"Anzahl der Einträge in der Tabelle 'minute_bar': {count:_}\")\n",
    "\n",
    "    # Daten aus der Tabelle 'minute_bar' abrufen und die letzten 5 Einträge ausgeben\n",
    "    minute_bars = session.query(MinuteBar).order_by(MinuteBar.date.desc()).limit(5).all()\n",
    "    if minute_bars:\n",
    "        for bar in reversed(minute_bars):  # reversed, um die Einträge in aufsteigender Reihenfolge anzuzeigen\n",
    "            print(f\"Date: {bar.date}, Symbol: {bar.symbol.ticker}, Open: {bar.open}, Close: {bar.close}, High: {bar.high}, Low: {bar.low}, Volume: {bar.volume}\")\n",
    "    else:\n",
    "        print(\"Keine Einträge in der Tabelle 'minute_bar' gefunden.\")\n",
    "else:\n",
    "    print(\"Die Tabelle 'minute_bar' existiert nicht in der Datenbank.\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_duplicates(csv_file_path):\n",
    "    bars1m = pd.read_csv(csv_file_path)\n",
    "    duplicates = bars1m.duplicated(subset=['date', 'ticker'])\n",
    "    if duplicates.any():\n",
    "        print(f\"Doppelte Einträge gefunden: {duplicates.sum():,}\")\n",
    "        print(bars1m[duplicates])\n",
    "    else:\n",
    "        print(\"Keine doppelten Einträge gefunden.\")\n",
    "\n",
    "def check_resampling_effect(bars1m):\n",
    "    original_count = len(bars1m)\n",
    "    bars1m_resampled = bars1m.reset_index().set_index('date').groupby('ticker').resample('1min').last().droplevel(0)\n",
    "    resampled_count = len(bars1m_resampled)\n",
    "    print(f\"Originale Anzahl der Einträge: {original_count:_}\")\n",
    "    print(f\"Anzahl der Einträge nach Resampling: {resampled_count:_}\")\n",
    "\n",
    "# Anzahl der Einträge in der CSV-Datei überprüfen\n",
    "csv_file_path = '/home/ageq/Git_Projects/MLdatalake/source/trimmed_file.csv'\n",
    "\n",
    "# Laden der Daten aus der CSV-Datei\n",
    "bars1m = load_data_from_csv(csv_file_path)\n",
    "print(f\"Anzahl der Einträge: {len(bars1m):,}\")\n",
    "\n",
    "print(\"Doppelte Einträge in der CSV-Datei überprüfen:\")\n",
    "check_for_duplicates(csv_file_path)\n",
    "\n",
    "print(\"Effekt des Resampling auf die Anzahl der Einträge überprüfen:\")\n",
    "check_resampling_effect(bars1m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##dauert zu lange\n",
    "\n",
    "from sqlalchemy import text\n",
    "\n",
    "# Sitzung erstellen\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "# Überprüfen, ob die Tabelle 'minute_bar' existiert\n",
    "if 'minute_bar' in table_names:\n",
    "    # Anzahl der Einträge in der Tabelle 'minute_bar' überprüfen\n",
    "    count = session.query(MinuteBar).count()\n",
    "    print(f\"Anzahl der Einträge in der Tabelle 'minute_bar': {count:_}\")\n",
    "\n",
    "    # Duplikate in der Tabelle 'minute_bar' suchen\n",
    "    query = text(\"\"\"\n",
    "        SELECT \n",
    "            date, symbol_id, COUNT(*) as dupe_count\n",
    "        FROM \n",
    "            minute_bar\n",
    "        GROUP BY \n",
    "            date, symbol_id\n",
    "        HAVING \n",
    "            COUNT(*) > 1\n",
    "    \"\"\")\n",
    "    \n",
    "    result = session.execute(query).fetchall()\n",
    "\n",
    "    if result:\n",
    "        print(f\"Anzahl der Duplikate: {len(result):_}\")\n",
    "        for row in result:\n",
    "            print(f\"Date: {row.date}, Symbol ID: {row.symbol_id}, Count: {row.dupe_coçunt}\")\n",
    "    else:\n",
    "        print(\"Keine Duplikate in der Tabelle 'minute_bar' gefunden.\")\n",
    "else:\n",
    "    print(\"Die Tabelle 'minute_bar' existiert nicht in der Datenbank.\")\n",
    "    \n",
    "    ##alternativ\n",
    "'''\n",
    "WITH recent_entries AS (\n",
    "    SELECT \n",
    "        date, \n",
    "        symbol_id\n",
    "    FROM \n",
    "        minute_bar\n",
    "    ORDER BY \n",
    "        date DESC\n",
    "    LIMIT 500000\n",
    ")\n",
    "SELECT \n",
    "    date, \n",
    "    symbol_id, \n",
    "    COUNT(*) as dupe_count\n",
    "FROM \n",
    "    recent_entries\n",
    "GROUP BY \n",
    "    date, \n",
    "    symbol_id\n",
    "HAVING \n",
    "    COUNT(*) > 1;\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import_data_notebook.ipynb\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from importlib import import_module\n",
    "\n",
    "# Ermitteln des aktuellen Verzeichnisses des Notebooks\n",
    "current_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "\n",
    "# Füge das Verzeichnis hinzu, in dem sich die Module befinden\n",
    "modules_dir = os.path.join(current_dir, 'modules')\n",
    "sys.path.append(modules_dir)\n",
    "\n",
    "# Importiere die Module\n",
    "create_database = import_module('create_database')\n",
    "extract_and_save_csv = import_module('extract_and_save_csv')\n",
    "import_to_db = import_module('import_to_db')\n",
    "\n",
    "# Lade die Konfigurationsdatei\n",
    "def load_config(config_file_path):\n",
    "    with open(config_file_path, 'r') as file:\n",
    "        config = json.load(file)\n",
    "    return config\n",
    "\n",
    "source = '/home/ageq/Git_Projects/MLdatalake/source/'\n",
    "\n",
    "# Beispiel für die Ausführung im Notebook\n",
    "config_file_path = os.path.join(current_dir, 'config.json')\n",
    "# Zielspeicherpfad\n",
    "csv_file_path = os.path.join(source, 'gespeicherter_dataframe.csv')\n",
    "symbol_filter = None  # Optional: Nur Daten für ein bestimmtes Symbol importieren, z.B. \"btcusd\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Erstelle die Datenbank und Tabellen\n",
    "create_database.main(config_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Extrahiere und speichere die CSV-Datei\n",
    "zip_file_path = os.path.join(source, 'trimmed_file.zip')\n",
    "extract_and_save_csv.extract_and_save_csv(zip_file_path, csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Importiere die Daten in die Datenbank\n",
    "import_to_db.main(config_file_path, csv_file_path, symbol_filter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
